{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport copy\nimport cv2\nimport glob\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nfrom operator import itemgetter\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport random\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import models, transforms\n\nrandom.seed(37)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-25T23:01:44.658411Z","iopub.execute_input":"2021-08-25T23:01:44.658799Z","iopub.status.idle":"2021-08-25T23:01:46.542337Z","shell.execute_reply.started":"2021-08-25T23:01:44.658717Z","shell.execute_reply":"2021-08-25T23:01:46.541495Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification'","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:01:55.695030Z","iopub.execute_input":"2021-08-25T23:01:55.695363Z","iopub.status.idle":"2021-08-25T23:01:55.701383Z","shell.execute_reply.started":"2021-08-25T23:01:55.695333Z","shell.execute_reply":"2021-08-25T23:01:55.700403Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We have several ideas for how to proceed, both 2D and 3D versions.  In order to properly test the 2D ones, it would be useful to have a single reference of all the good (as in, not blank) images *in order*, so that random or midpoint selection can easily grab them without having to process everything.  Ideally we can save this in a JSON file, where the main keys are patient IDs and the interior keys are modalities, with the inner values being ordered lists.","metadata":{}},{"cell_type":"markdown","source":"Several of these helper functions came from or were inspired by this notebook: https://www.kaggle.com/furcifer/no-baseline-pytorch-cnn-for-mri?scriptVersionId=68186710\n","metadata":{}},{"cell_type":"code","source":"#This is from the furcifer notebook\ndef _dicom2array(path, voi_lut=True, fix_monochrome=True, resize=False):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    #Normalize the data: subtract off the minimum, divide by the maximum, convert to 256 uint8\n    data = data - np.min(data)\n    data = data/np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    #Resize images to target value\n    if resize:\n        data = cv2.resize(data, (256, 256))\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:02:02.476346Z","iopub.execute_input":"2021-08-25T23:02:02.476666Z","iopub.status.idle":"2021-08-25T23:02:02.485733Z","shell.execute_reply.started":"2021-08-25T23:02:02.476636Z","shell.execute_reply":"2021-08-25T23:02:02.484890Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"If one looks at the raw images, one sees lots of blank space around the actually brain, which is not useful for classification.  The following function finds the edges of the brain and eliminates the surrounding blank space.  Note: this is circumscribing it, not eliminating *all* the blank space, there is still blank space in the corners because PyTorch needs a cuboid input.","metadata":{}},{"cell_type":"code","source":"#This cell was used to create the JSON file containing all of the images per modality in order with the blanks stripped out\n#It also identified the \"weirds\", which are the patients where the DICOM images in at least one modality do not have the usual\n#complement of features.  I'm not sure what to do with them at this time, so for now they are being excluded.\n\nfull_data_dict = {}\nmodes = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\npatients = glob.glob(f'{train_path}/train/*')\npatient_IDs = sorted([p.split('/')[-1] for p in patients])\nweirds = ['00109', '00157', '00170', '00186', '00353', '00367', '00414', '00561', '00563', '00564', '00565',\n         '00756', '00834', '00839']\nprint('Number of Patients: ', len(patient_IDs))\nprint('First patient: ', patient_IDs[0])\nprint('Last patient: ', patient_IDs[-1])\nfor patient in patient_IDs:\n    if patient in weirds:\n        continue\n    print('Patient ID: ', patient)\n    subdict = {}\n    for mode in modes:\n        imgnames = glob.glob(f'{train_path}/train/{patient}/{mode}/*.dcm')\n        if len(imgnames) == 0:\n            subdict[mode] = []\n            continue\n        image_slice_locs = [pydicom.dcmread(im)[('0020', '1041')].value for im in imgnames]\n        imgpairs = zip(imgnames, image_slice_locs)\n        imgpairs = sorted(imgpairs, key=itemgetter(1))\n        \n        images = [f[0] for f in imgpairs]\n        #print('Number of initial %s images: %i' % (mode, len(images)))\n        real_images = [_dicom2array(f) for f in images]\n        good_indices = [idx for idx,image in enumerate(real_images) if np.max(image) > 0]\n        good_imnames = [im for idx,im in enumerate(images) if idx in good_indices]\n        if len(good_imnames) == 0:\n            subdict[mode] = []\n            continue\n        \n        final_imnames = [imname.split('/')[-1] for imname in good_imnames]\n    \n        #print('Number of final images: ', len(final_imnames))\n        subdict[mode] = final_imnames\n    full_data_dict[patient] = subdict","metadata":{"execution":{"iopub.status.busy":"2021-08-18T15:36:46.22051Z","iopub.execute_input":"2021-08-18T15:36:46.22113Z","iopub.status.idle":"2021-08-18T16:37:29.980109Z","shell.execute_reply.started":"2021-08-18T15:36:46.221085Z","shell.execute_reply":"2021-08-18T16:37:29.975826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is the cell that actually wrote the JSON file, using the dictionary from the above cell\n#with open('all_valid_image_names.json', 'w') as outfile:\n#    json.dump(full_data_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:30:23.068695Z","iopub.execute_input":"2021-08-18T20:30:23.069073Z","iopub.status.idle":"2021-08-18T20:30:23.09275Z","shell.execute_reply.started":"2021-08-18T20:30:23.069041Z","shell.execute_reply":"2021-08-18T20:30:23.091297Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Access the JSON file with all of the sorted good images\nf = open('../input/brain-tumor-valid-images-in-order/all_valid_image_names.json')\ngood_image_names = json.load(f)\nprint('Number of Patients with good images: ', len(good_image_names))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:02:13.732922Z","iopub.execute_input":"2021-08-25T23:02:13.733244Z","iopub.status.idle":"2021-08-25T23:02:13.855064Z","shell.execute_reply.started":"2021-08-25T23:02:13.733215Z","shell.execute_reply":"2021-08-25T23:02:13.854170Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of Patients with good images:  571\n","output_type":"stream"}]},{"cell_type":"code","source":"#Eliminate as much blank space around a 2D slice as possible\ndef _circumscriber2D(img: np.array) -> np.array:\n    vmin = 0\n    hmin = 0\n    vlimit, hlimit = img.shape\n    \n    for i in range(vlimit):\n        if np.max(img[i, :]) == 0:\n            vmin += 1\n        else:\n            break\n    vmax = vmin + 1\n    for i in range(vmin+1, vlimit):\n        if np.max(img[i, :]) > 0:\n            vmax += 1\n        else:\n            break\n\n    for j in range(hlimit):\n        if np.max(img[:, j]) == 0:\n            hmin += 1\n        else:\n            break\n    hmax = hmin + 1\n    for j in range(hmin+1, hlimit):\n        if np.max(img[:, j]) > 0:\n            hmax += 1\n        else:\n            break\n    return img[vmin: vmax, hmin:hmax]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:02:18.148737Z","iopub.execute_input":"2021-08-25T23:02:18.149048Z","iopub.status.idle":"2021-08-25T23:02:18.159493Z","shell.execute_reply.started":"2021-08-25T23:02:18.149018Z","shell.execute_reply":"2021-08-25T23:02:18.158520Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Eliminate as much blank space around a 3D cube as possible.  This one assumes the purely blank slices were already filtered out\ndef _circumscriber3D(img: np.array) -> np.array:\n    #First is vertical, second is horizontal, third is slices\n    vmin = 0\n    vlimit = img.shape[0]\n    hmin = 0\n    hlimit = img.shape[1]\n    \n    for i in range(vlimit):\n        if np.max(img[i, :, :]) == 0:\n            vmin += 1\n        else:\n            break\n    vmax = vmin + 1\n    for i in range(vmin+1, vlimit):\n        if np.max(img[i, :, :]) > 0:\n            vmax += 1\n        else:\n            break\n    \n    for j in range(hlimit):\n        if np.max(img[:, j, :]) == 0:\n            hmin += 1\n        else:\n            break\n    hmax = hmin + 1\n    for j in range(hmin+1, hlimit):\n        if np.max(img[:, j, :]) > 0:\n            hmax += 1\n        else:\n            break\n    return img[vmin:vmax, hmin:hmax, :]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:02:29.512245Z","iopub.execute_input":"2021-08-25T23:02:29.512737Z","iopub.status.idle":"2021-08-25T23:02:29.540962Z","shell.execute_reply.started":"2021-08-25T23:02:29.512695Z","shell.execute_reply":"2021-08-25T23:02:29.540009Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"This is the Cutout data augmenter: when active, it will randomly zero out cubes of the brain image.  My guess is that, with the proper block size ('side' parameter), this will be a more effective regularizer than SliceSkip, but I could be mistaken.\nBesides the size of the blocks, the number of dropped blocks will likely also matter a fair bit, but I'll add that functionality later.  I have changed the name from DropBlock to Cutout in keeping with the convention described in the DropBlock paper: https://arxiv.org/pdf/1810.12890.pdf\nThe full DropBlock regularizer applies this to all the feature maps produced by the convolutional layers, whereas what I'm doing here is making random changes to the input.","metadata":{}},{"cell_type":"code","source":"def _Cutout(img: np.array, side: int) -> np.array:\n    ycap = img.shape[0] - side\n    xcap = img.shape[1] - side\n    \n    nholes = random.randint(1, 5)\n    ycorners = random.sample(range(0, ycap), nholes)\n    xcorners = random.sample(range(0, xcap), nholes)\n    \n    if len(img.shape) == 3:\n        dcap = img.shape[2] - side\n        dcorners = random.sample(range(0, dcap), nholes)\n    \n    for i in range(nholes):\n        if len(img.shape) == 3:\n            img[ycorners[i]:ycorners[i]+side, xcorners[i]:xcorners[i]+side, dcorners[i]:dcorners[i]+side] = 0\n        else:\n            img[ycorners[i]:ycorners[i]+side, xcorners[i]:xcorners[i]+side] = 0\n    \n    return img","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:02:38.990950Z","iopub.execute_input":"2021-08-25T23:02:38.991274Z","iopub.status.idle":"2021-08-25T23:02:38.998658Z","shell.execute_reply.started":"2021-08-25T23:02:38.991245Z","shell.execute_reply":"2021-08-25T23:02:38.997739Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def load_single_slice(patient_id, split='train', modality='FLAIR', position=0):\n    \"\"\"\n    Load in a single slice for a given patient, of a given modality and position within the line-up\n    \"\"\"\n    if split != 'train' and split != 'test':\n        print('Please request a valid split: train or test. Defaulting to train')\n        split='train'\n        \n    if modality not in ['FLAIR', 'T1w', 'T1wCE', 'T2w']:\n        print('Please choose a valid modality: FLAIR, T1w, T1wCE, or T2w. Defaulting to FLAIR')\n        modality = 'FLAIR'\n        \n    available_images = good_image_names[patient_id][modality]\n    if len(available_images) == 0:\n        print('There doesnt seem to be anything here! Lets go back and tell Master Luke...')\n        print('Patient ID: ', patient_id)\n        print('Modality: ', modality)\n        print('available images: ', available_images)\n        return None\n    \n    if position == 'middle':\n        position = int(len(available_images) * 0.5)\n    \n    #print('Number of Available Images: ', len(available_images))\n    if position >= len(available_images):\n        print('Please choose an acceptable position, must be no more than: ', len(available_images) - 1)\n        print('Defaulting to middle position')\n        position = int(len(available_images) * 0.5)\n\n    imname = available_images[position]\n    #print('imname: ', imname)\n    image_path = f'{train_path}/{split}/{patient_id}/{modality}/{imname}'\n    image = glob.glob(image_path)[0]\n    real_image = _dicom2array(image)\n    #print('real_image shape: ', real_image.shape)\n    final_image = _circumscriber2D(real_image).T\n    #print('final_image shape: ', final_image.shape)\n    return final_image\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:56:53.639441Z","iopub.execute_input":"2021-08-25T23:56:53.639813Z","iopub.status.idle":"2021-08-25T23:56:53.647755Z","shell.execute_reply.started":"2021-08-25T23:56:53.639780Z","shell.execute_reply":"2021-08-25T23:56:53.646763Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"test709 = load_single_slice('00709', split='train', modality='FLAIR', position=90)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:56:58.786073Z","iopub.execute_input":"2021-08-25T23:56:58.786381Z","iopub.status.idle":"2021-08-25T23:56:58.794519Z","shell.execute_reply.started":"2021-08-25T23:56:58.786351Z","shell.execute_reply":"2021-08-25T23:56:58.793617Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"There doesnt seem to be anything here! Lets go back and tell Master Luke...\nPatient ID:  00709\nModality:  FLAIR\navailable images:  []\n","output_type":"stream"}]},{"cell_type":"code","source":"#Test out load_single_slice\nimage_zero = load_single_slice('00000', split='train', modality='FLAIR', position=128)\nprint('image_zero shape: ', image_zero.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:41:00.820473Z","iopub.execute_input":"2021-08-25T19:41:00.821068Z","iopub.status.idle":"2021-08-25T19:41:00.862481Z","shell.execute_reply.started":"2021-08-25T19:41:00.821000Z","shell.execute_reply":"2021-08-25T19:41:00.861339Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"image_zero shape:  (275, 270)\n","output_type":"stream"}]},{"cell_type":"code","source":"#This was inspired by load_rand_dicom_images in the furcifer notebook\ndef load_FULL_brain(scan_id, split = 'train', modality='FLAIR', sliceSkip=0.0, cutOut=False):\n    \"\"\"\n    send all of the images in the chosen modality, in order, as a single 3D np array\n    \"\"\"\n    if split != \"train\" and split != \"test\":\n        print('Please request a valid split: train or test.  Defaulting to train.')\n        split = \"train\"\n        \n    if modality != 'FLAIR' and modality != 'T1w' and modality != 'T1wCE' and modality != 'T2w':\n        print('Please select an appropriate modality: FLAIR, T1w, T1wCE, or T2w')\n        print('Defaulting to FLAIR')\n        modality = 'FLAIR'\n        \n    if sliceSkip >= 1:\n        sliceSkip = 0\n        print('Please choose a valid sliceSkip number, from 0 to 1 inclusive/exclusive')\n        \n    image = sorted(glob.glob(f'{train_path}/{split}/{scan_id}/{modality}/*.dcm'))\n    nKeep = int(len(image) * (1 - sliceSkip))\n    image = random.sample(image, nKeep)\n    \n    image_slice_locs = [pydicom.dcmread(im)[('0020', '1041')].value for im in image]\n    image_pairs = list(zip(image, image_slice_locs))\n    ordered_IP = sorted(image_pairs, key=itemgetter(1))\n    images = [f[0] for f in ordered_IP]\n    real_images = [_dicom2array(f) for f in images]\n    good_images = np.array([im for im in real_images if np.max(im) > 0]).T\n    final_image = _circumscriber3D(good_images)\n    \n    #Only use Cutout data augmentation if explicitly desired\n    if cutOut:\n        final_image = _Cutout(final_image, 10)\n    \n    return final_image","metadata":{"execution":{"iopub.status.busy":"2021-08-25T20:26:10.674120Z","iopub.execute_input":"2021-08-25T20:26:10.674481Z","iopub.status.idle":"2021-08-25T20:26:10.684652Z","shell.execute_reply.started":"2021-08-25T20:26:10.674451Z","shell.execute_reply":"2021-08-25T20:26:10.683621Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#This was copy-pasted from the furcifer notebook\ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(512,512)):\n    rows = len(imgs)//cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i in range(4):\n        img = imgs[:,:,i]\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:03:02.065660Z","iopub.execute_input":"2021-08-25T23:03:02.066009Z","iopub.status.idle":"2021-08-25T23:03:02.071662Z","shell.execute_reply.started":"2021-08-25T23:03:02.065980Z","shell.execute_reply":"2021-08-25T23:03:02.070759Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"image_zero = load_single_slice('00000', split='train', modality='FLAIR', position=100)\nimage_one = load_single_slice('00000', split='train', modality='FLAIR', position=101)\nimage_two = load_single_slice('00000', split='train', modality='FLAIR', position=102)\nimage_three = load_single_slice('00000', split='train', modality='FLAIR', position=103)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T21:07:42.369248Z","iopub.execute_input":"2021-08-18T21:07:42.369737Z","iopub.status.idle":"2021-08-18T21:07:42.48988Z","shell.execute_reply.started":"2021-08-18T21:07:42.369697Z","shell.execute_reply":"2021-08-18T21:07:42.488288Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boxtest = load_FULL_brain('00000', split='train', modality='FLAIR', sliceSkip=0.0, cutOut=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:54:36.950925Z","iopub.execute_input":"2021-08-23T21:54:36.951277Z","iopub.status.idle":"2021-08-23T21:54:43.991045Z","shell.execute_reply.started":"2021-08-23T21:54:36.951249Z","shell.execute_reply":"2021-08-23T21:54:43.989972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at four slices from the middle of patient 00000's FLAIR brain image.","metadata":{}},{"cell_type":"code","source":"plot_imgs(boxtest[:,:,100:104])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:54:47.957219Z","iopub.execute_input":"2021-08-23T21:54:47.957581Z","iopub.status.idle":"2021-08-23T21:54:48.907711Z","shell.execute_reply.started":"2021-08-23T21:54:47.957552Z","shell.execute_reply":"2021-08-23T21:54:48.906354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SliceLoader(Dataset):\n    def __init__(self, label_file, path, split, modality, position=None, exclude=None, val_split=0.25):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        self.split = split\n        self.modality = modality\n        self.position = position\n        self.exclude = exclude\n        splitdir = 'train'\n        if self.split == 'test':\n            splitdir = 'test'\n        self.ids = [a.split('/')[-1] for a in sorted(glob.glob(path + f'/{splitdir}/*'))]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]\n            \n        self.ids = [a for a in self.ids if a not in self.exclude]\n            \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        if not self.position:\n            available_images = good_image_names[p_id][self.modality]\n            position = int(len(available_images) * 0.5) #Grab the middle slice if one not selected\n        else:\n            position = self.position\n            \n        if self.split == 'val':\n            self.split = 'train'\n        image = load_single_slice(p_id, self.split, self.modality, position)\n        image_triple = np.stack([image, image, image]).T\n        transform = transforms.Compose([transforms.ToTensor()])\n        image = transform(image_triple)\n        \n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(image, dtype=torch.float32), label\n        return torch.tensor(image, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:46:25.115948Z","iopub.execute_input":"2021-08-25T23:46:25.116276Z","iopub.status.idle":"2021-08-25T23:46:25.130344Z","shell.execute_reply.started":"2021-08-25T23:46:25.116245Z","shell.execute_reply":"2021-08-25T23:46:25.129090Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"weirds = ['00109', '00157', '00170', '00186', '00353', '00367', '00414', '00561', '00563', '00564', '00565', '00709',\n         '00756', '00834', '00839']","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:01:08.844559Z","iopub.execute_input":"2021-08-26T00:01:08.844914Z","iopub.status.idle":"2021-08-26T00:01:08.849132Z","shell.execute_reply.started":"2021-08-26T00:01:08.844884Z","shell.execute_reply":"2021-08-26T00:01:08.848066Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Let's see if SliceLoader can actually load slices\ntrain_bs = 1\ntrain_dataset = SliceLoader('train_labels.csv', train_path, split='train', modality='FLAIR', position=17, exclude=weirds)\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n\nfor img, label in train_loader:\n    print('---------------Iteration---------------------')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:41:45.236636Z","iopub.execute_input":"2021-08-25T19:41:45.236996Z","iopub.status.idle":"2021-08-25T19:41:45.412384Z","shell.execute_reply.started":"2021-08-25T19:41:45.236968Z","shell.execute_reply":"2021-08-25T19:41:45.411642Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"---------------Iteration---------------------\ntorch.Size([1, 3, 162, 189])\ntensor(0.)\ntensor(0.4503)\ntensor(1.)\ntorch.Size([1])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","output_type":"stream"}]},{"cell_type":"code","source":"labels = pd.read_csv(os.path.join(train_path, 'train_labels.csv'))\nlabels.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T22:30:08.789763Z","iopub.execute_input":"2021-08-18T22:30:08.790189Z","iopub.status.idle":"2021-08-18T22:30:08.817883Z","shell.execute_reply.started":"2021-08-18T22:30:08.790151Z","shell.execute_reply":"2021-08-18T22:30:08.816584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was inspired by the BrainTumor Dataset class in the furcifer notebook, though I have heavily modified it.\nclass BrainLoader(Dataset):\n    def __init__(self, label_file, path, split, modality, skip=0.0, cutout=False, val_split=0.25):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        splitdir = 'train'\n        self.split = split\n        if self.split == 'test':\n            splitdir = 'test'\n        self.modality = modality\n        \n        self.skip = skip\n        self.cutout = cutout\n        if self.split != 'train':\n            self.skip = 0\n            self.drop = cutout\n        \n        self.ids = [a.split('/')[-1] for a in sorted(glob.glob(path + f'/{splitdir}/*'))]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]\n            \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        imgs = load_FULL_brain(p_id, split=self.split, modality=self.modality, sliceSkip=self.skip, cutOut=self.cutout)\n        transform = transforms.Compose([transforms.ToTensor()])\n        imgs = transform(imgs)\n        \n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(imgs, dtype=torch.float32), label\n        return torch.tensor(imgs, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:42:28.714971Z","iopub.execute_input":"2021-08-25T19:42:28.715502Z","iopub.status.idle":"2021-08-25T19:42:28.729146Z","shell.execute_reply.started":"2021-08-25T19:42:28.715459Z","shell.execute_reply":"2021-08-25T19:42:28.728368Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's test out BrainLoader.","metadata":{}},{"cell_type":"code","source":"train_bs = 1\ntrain_dataset = BrainLoader('train_labels.csv', train_path, split='train', modality='T1w', skip=0.0, cutout=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n\nfor img, label in train_loader:\n    print('Iteration')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:42:42.773266Z","iopub.execute_input":"2021-08-25T19:42:42.773857Z","iopub.status.idle":"2021-08-25T19:42:44.578645Z","shell.execute_reply.started":"2021-08-25T19:42:42.773810Z","shell.execute_reply":"2021-08-25T19:42:44.577650Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n  if sys.path[0] == '':\n","output_type":"stream"},{"name":"stdout","text":"Iteration\ntorch.Size([1, 143, 140, 172])\ntensor(0.)\ntensor(0.2631)\ntensor(1.)\ntorch.Size([1])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Alright, now that we have the loader pretty well set up, we need an actual model or four to train.  This seems like a valuable source: https://pytorch.org/hub/ ","metadata":{}},{"cell_type":"code","source":"#Simple Resnet-18 model, pretrained on ImageNet.  Let's see what it can do...\nmodel_dummy = models.resnet18(pretrained=True)\nnum_ftrs = model_dummy.fc.in_features\n# Here the size of each output sample is set to 2.\nmodel_dummy.fc = nn.Linear(num_ftrs, 2)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:55:56.566537Z","iopub.execute_input":"2021-08-23T21:55:56.567061Z","iopub.status.idle":"2021-08-23T21:56:03.981174Z","shell.execute_reply.started":"2021-08-23T21:55:56.567021Z","shell.execute_reply":"2021-08-23T21:56:03.980405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet50 = models.resnet50(pretrained=True)\nnum_ftrs = model_resnet50.fc.in_features\nmodel_resnet50.fc = nn.Linear(num_ftrs, 2)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:03:42.123522Z","iopub.execute_input":"2021-08-25T23:03:42.123884Z","iopub.status.idle":"2021-08-25T23:03:49.642961Z","shell.execute_reply.started":"2021-08-25T23:03:42.123854Z","shell.execute_reply":"2021-08-25T23:03:49.642148Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab0f49da17a489493a28cec65bc8828"}},"metadata":{}}]},{"cell_type":"code","source":"path709 = f'{train_path}/train/00709/'\nimages709 = glob.glob(f'{train_path}/train/00709/FLAIR/*.dcm')\nprint('Number of FLAIR images for Patient 00709: ', len(images709))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:53:55.566145Z","iopub.execute_input":"2021-08-25T23:53:55.566477Z","iopub.status.idle":"2021-08-25T23:53:55.618286Z","shell.execute_reply.started":"2021-08-25T23:53:55.566445Z","shell.execute_reply":"2021-08-25T23:53:55.617288Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Number of FLAIR images for Patient 00709:  192\n","output_type":"stream"}]},{"cell_type":"code","source":"train_bs = 1\nval_bs = 1\ntrain_dummy = SliceLoader('train_labels.csv', train_path, split='train', modality='FLAIR', position='middle', exclude=weirds)\nval_dummy = SliceLoader('train_labels.csv', train_path, split='val', modality='FLAIR', position='middle', exclude=weirds)\ntrain_loader = DataLoader(train_dummy, batch_size=train_bs, shuffle=True)\nval_loader = DataLoader(val_dummy, batch_size=val_bs, shuffle=False)\n\ndataloaders = {'train': train_loader, 'val': val_loader}\n\nn_images = 0\nn_rand = 0\nn_correct = 0\n\nfor img, label in val_loader:\n    n_images += 1\n    if n_images > 10:\n        break\n    #print('img shape: ', img.shape)\n\n    out = model_resnet50(img)\n    print('out: ', out)\n    _,pred = torch.max(out, dim=1)\n    randpred = random.choice([0,1])\n    print('pred: ', pred)\n    print('out[0][pred]: ', out[0][pred])\n    if pred == label:\n        n_correct += 1\n    if randpred == label:\n        n_rand += 1\n        \nprint('Correct: ', n_correct)\nprint('Random Correct: ', n_rand)\nprint('Total: ', n_images)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:01:29.507022Z","iopub.execute_input":"2021-08-26T00:01:29.507350Z","iopub.status.idle":"2021-08-26T00:01:29.583069Z","shell.execute_reply.started":"2021-08-26T00:01:29.507320Z","shell.execute_reply":"2021-08-26T00:01:29.580883Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-55013e6cb262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#print('img shape: ', img.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_resnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"],"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same","output_type":"error"}]},{"cell_type":"markdown","source":"Alright, let's add a training loop, without a training loop we don't really have anything.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndataset_sizes = {'train': 427, 'val': 143}\ncriterion = nn.BCEWithLogitsLoss()\noptimizer_ft = optim.Adam(model_resnet50.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T23:05:27.611077Z","iopub.execute_input":"2021-08-25T23:05:27.611416Z","iopub.status.idle":"2021-08-25T23:05:27.661266Z","shell.execute_reply.started":"2021-08-25T23:05:27.611385Z","shell.execute_reply":"2021-08-25T23:05:27.660421Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, num_epochs, device='cpu'):\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0\n            model = model.to(device)\n            for img, label in dataloaders[phase]:\n                #Send everything to the relevant device\n                img = img.to(device)\n                label = label.float()\n                label = label.to(device)\n                \n                #Zero the parameter gradients\n                optimizer.zero_grad()\n                \n                #Forward pass, track history only in train mode\n                with torch.set_grad_enabled(phase == 'train'):\n                    output = model(img)\n                    _, pred = torch.max(output, dim=1)\n                    loss = criterion(output[0][pred], label)\n                 \n                \n                    #Backward pass, optimize only in training mode\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                # statistics\n                running_loss += loss.item() * img.size(0)\n                running_corrects += torch.sum(pred == label.data)\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n            phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n    \n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:01:41.564084Z","iopub.execute_input":"2021-08-26T00:01:41.564406Z","iopub.status.idle":"2021-08-26T00:01:41.575483Z","shell.execute_reply.started":"2021-08-26T00:01:41.564375Z","shell.execute_reply":"2021-08-26T00:01:41.574452Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"FT_model = train_model(model_resnet50, criterion, optimizer_ft, num_epochs=25, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T00:01:51.049304Z","iopub.execute_input":"2021-08-26T00:01:51.049635Z","iopub.status.idle":"2021-08-26T00:19:11.658402Z","shell.execute_reply.started":"2021-08-26T00:01:51.049603Z","shell.execute_reply":"2021-08-26T00:19:11.657551Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 0/24\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.7011 Acc: 0.5210\nval Loss: 0.6954 Acc: 0.5524\nEpoch 1/24\n----------\ntrain Loss: 0.7032 Acc: 0.5210\nval Loss: 0.7037 Acc: 0.5524\nEpoch 2/24\n----------\ntrain Loss: 0.6905 Acc: 0.5210\nval Loss: 0.6955 Acc: 0.5524\nEpoch 3/24\n----------\ntrain Loss: 0.6846 Acc: 0.5210\nval Loss: 0.6974 Acc: 0.5524\nEpoch 4/24\n----------\ntrain Loss: 0.6873 Acc: 0.5210\nval Loss: 0.7058 Acc: 0.5524\nEpoch 5/24\n----------\ntrain Loss: 0.6927 Acc: 0.5210\nval Loss: 0.7328 Acc: 0.5524\nEpoch 6/24\n----------\ntrain Loss: 0.6794 Acc: 0.5210\nval Loss: 0.7056 Acc: 0.5524\nEpoch 7/24\n----------\ntrain Loss: 0.6683 Acc: 0.5210\nval Loss: 0.7204 Acc: 0.5524\nEpoch 8/24\n----------\ntrain Loss: 0.6542 Acc: 0.5210\nval Loss: 0.7073 Acc: 0.5524\nEpoch 9/24\n----------\ntrain Loss: 0.6497 Acc: 0.5210\nval Loss: 0.7373 Acc: 0.5524\nEpoch 10/24\n----------\ntrain Loss: 0.6181 Acc: 0.5210\nval Loss: 0.7292 Acc: 0.5524\nEpoch 11/24\n----------\ntrain Loss: 0.5576 Acc: 0.5210\nval Loss: 0.8016 Acc: 0.5524\nEpoch 12/24\n----------\ntrain Loss: 0.5525 Acc: 0.5210\nval Loss: 0.7902 Acc: 0.5524\nEpoch 13/24\n----------\ntrain Loss: 0.4244 Acc: 0.5234\nval Loss: 0.8718 Acc: 0.5524\nEpoch 14/24\n----------\ntrain Loss: 0.2815 Acc: 0.5210\nval Loss: 0.8404 Acc: 0.5524\nEpoch 15/24\n----------\ntrain Loss: 0.2424 Acc: 0.5234\nval Loss: 0.9828 Acc: 0.5524\nEpoch 16/24\n----------\ntrain Loss: 0.1594 Acc: 0.5234\nval Loss: 1.1387 Acc: 0.5524\nEpoch 17/24\n----------\ntrain Loss: 0.0641 Acc: 0.5257\nval Loss: 0.7965 Acc: 0.5524\nEpoch 18/24\n----------\ntrain Loss: 0.0112 Acc: 0.5210\nval Loss: 0.7647 Acc: 0.5524\nEpoch 19/24\n----------\ntrain Loss: 0.0048 Acc: 0.5234\nval Loss: 1.3882 Acc: 0.5524\nEpoch 20/24\n----------\ntrain Loss: 0.0032 Acc: 0.5234\nval Loss: 0.7496 Acc: 0.5524\nEpoch 21/24\n----------\ntrain Loss: 0.0023 Acc: 0.5234\nval Loss: 0.7481 Acc: 0.5524\nEpoch 22/24\n----------\ntrain Loss: 0.0017 Acc: 0.5210\nval Loss: 1.2337 Acc: 0.5524\nEpoch 23/24\n----------\ntrain Loss: 0.0013 Acc: 0.5234\nval Loss: 2.1894 Acc: 0.5594\nEpoch 24/24\n----------\ntrain Loss: 0.0010 Acc: 0.5257\nval Loss: 0.9472 Acc: 0.5524\nBest val Acc: 0.559441\n","output_type":"stream"}]}]}