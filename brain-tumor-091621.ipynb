{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Participation in this competition helps contribute to a meaningful cause \nAs mentioned in the competition overview, \"If successful, you'll help brain cancer patients receive less invasive diagnoses and treatments. The introduction of new and customized treatment strategies before surgery has the potential to improve the management, survival, and prospects of patients with brain cancer.\"","metadata":{}},{"cell_type":"markdown","source":"**Dataset**\n\nThe dataset is divided as follows: Each independent case has a dedicated folder identified by a five-digit number. Within each of these “case” folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format. The exact mpMRI scans included are:\n\n* Fluid Attenuated Inversion Recovery (FLAIR)\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n\n**Files**\n\n* train/ - folder containing the training files, with each top-level folder representing a subject. \n* train_labels.csv - file containing the target MGMT_value for each subject in the training data (e.g. the presence of MGMT promoter methylation)\n* test/ - the test files, which use the same structure as train/; your task is to predict the MGMT_value for each subject in the test data. NOTE: the total size of the rerun test set (Public and Private) is ~5x the size of the Public test set\n* sample_submission.csv - a sample submission file in the correct format\n\nEach independent case is labeled with MGMT_value. 1 corresponds to the presence of MGMT (tumor) and 0 corresponds to absense.","metadata":{}},{"cell_type":"markdown","source":"## Understanding Dataset\n\nFor any Data Science project, it is crucial to understand the data, as best as possible, using visualization and Exploratory Data Analysis (EDA) techniques. \n\nLet's start with understanding the training labels file. But before that, let's make a cell to keep all the imported libraries in one place.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom collections import Counter\nfrom collections import OrderedDict\nimport copy\nimport csv\nimport cv2\nimport glob\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nfrom operator import itemgetter\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport random\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import models, transforms\n\nrandom.seed(37)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-16T19:53:00.369184Z","iopub.execute_input":"2021-09-16T19:53:00.3695Z","iopub.status.idle":"2021-09-16T19:53:06.110489Z","shell.execute_reply.started":"2021-09-16T19:53:00.369426Z","shell.execute_reply":"2021-09-16T19:53:06.109588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EDA code taken from awesome work at https://www.kaggle.com/ihelon/brain-tumor-eda-with-animations-and-modeling","metadata":{}},{"cell_type":"code","source":"#exploring distribution of training labels file\ntrain_df = pd.read_csv(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:53:27.497239Z","iopub.execute_input":"2021-09-16T19:53:27.497697Z","iopub.status.idle":"2021-09-16T19:53:27.530178Z","shell.execute_reply.started":"2021-09-16T19:53:27.497662Z","shell.execute_reply":"2021-09-16T19:53:27.529215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.countplot(data=train_df, x=\"MGMT_value\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's a near even split of data between tumourous and non-tumorous patients. This is nice as we don't need to perform stratification of data.  This gives us the simplest possible classifier: one which always assigns MGMT = 1 to every patient, because it's the slightly more common one.  Such a classifer would achieve an accuracy of 307/585 or 52.5% accuracy, nothing to write home about.","metadata":{}},{"cell_type":"code","source":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\n\ndef visualize_sample(\n    brats21id, \n    slice_i,\n    mgmt_value,\n    types=(\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\")\n):\n    plt.figure(figsize=(16, 5))\n    patient_path = os.path.join(\n        \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\", \n        str(brats21id).zfill(5),\n    )\n    for i, t in enumerate(types, 1):\n        t_paths = sorted(\n            glob.glob(os.path.join(patient_path, t, \"*\")), \n            key=lambda x: int(x[:-4].split(\"-\")[-1]),\n        )\n        data = load_dicom(t_paths[int(len(t_paths) * slice_i)])\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{t}\", fontsize=16)\n        plt.axis(\"off\")\n\n    plt.suptitle(f\"MGMT_value: {mgmt_value}\", fontsize=16)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:53:42.723965Z","iopub.execute_input":"2021-09-16T19:53:42.724684Z","iopub.status.idle":"2021-09-16T19:53:42.736378Z","shell.execute_reply.started":"2021-09-16T19:53:42.724649Z","shell.execute_reply":"2021-09-16T19:53:42.735718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in random.sample(range(train_df.shape[0]), 10):\n    _brats21id = train_df.iloc[i][\"BraTS21ID\"]\n    _mgmt_value = train_df.iloc[i][\"MGMT_value\"]\n    visualize_sample(brats21id=_brats21id, mgmt_value=_mgmt_value, slice_i=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T17:03:14.745676Z","iopub.execute_input":"2021-09-16T17:03:14.745953Z","iopub.status.idle":"2021-09-16T17:03:19.398558Z","shell.execute_reply.started":"2021-09-16T17:03:14.745926Z","shell.execute_reply":"2021-09-16T17:03:19.39788Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some images have apparent tumorous growth, whereas many others are hard to detect by a human eye. This is where ML based techniques can help us find patterns not fully clear to a human eye.  Before proceeding to the full-scale ML analysis of the images, let's see how effective a classifier we can make using only the number of images of each modality for the patient in question.  Note, this is very important: the number of images of each type should tell us *absolutely nothing* about the MGMT status of the patient, because the number of images is determined by the settings on the MRI machine prior to anyone having any knowledge about the type of tumor the patient is afflicted with.  Nevertheless, there may be some spurious correlations that an algorithm could pick up, which could lead to errors when used to analyze images from new patients.","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification'\ntrain_df = pd.read_csv(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:54:03.538773Z","iopub.execute_input":"2021-09-16T19:54:03.539056Z","iopub.status.idle":"2021-09-16T19:54:03.552604Z","shell.execute_reply.started":"2021-09-16T19:54:03.539028Z","shell.execute_reply":"2021-09-16T19:54:03.551837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Write out a csv file with 6 columns: Patient ID, # FLAIR images, # T1w images, # T1wCE images, # T2w images, MGMT type\nfull_list = []\nIDs = train_df['BraTS21ID'].values\nMGMTs = train_df['MGMT_value'].values\nlabels = {str(ID).zfill(5): MGMT for ID, MGMT in zip(IDs, MGMTs)}\npatients = glob.glob(f'{train_path}/train/*')\npatient_IDs = sorted([p.split('/')[-1] for p in patients])\nmodes = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\nfor p in patient_IDs:\n    subdict = {}\n    subdict['Patient'] = p\n    subdict['MGMT'] = labels[p]\n    for mode in modes:\n        imgnames = glob.glob(f'{train_path}/train/{p}/{mode}/*.dcm')\n        subdict[mode] = len(imgnames)\n    full_list.append(subdict)\n    #print('subdict: ', subdict)\n\nwith open('patient_image_count.csv', mode='w') as image_count_file:\n    fieldnames = ['Patient', 'FLAIR', 'T1w', 'T1wCE', 'T2w', 'MGMT']\n    image_writer = csv.DictWriter(image_count_file, fieldnames=fieldnames)\n    \n    image_writer.writeheader()\n    for f in full_list:\n        image_writer.writerow(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_df = pd.read_csv('patient_image_count.csv')\nX = simple_df[['FLAIR', 'T1w', 'T1wCE', 'T2w']]\ny = simple_df['MGMT']\nprint(simple_df.head(10))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=37)\n#print('X_train shape: ', X_train.shape)\n#print('y_test shape: ', y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('------------SVM--------------')\nsvc = SVC()\nsvc.fit(X_train, y_train)\nsvc_prediction = svc.predict(X_test)\n#print(confusion_matrix(y_test, svc_prediction))\nprint(classification_report(y_test, svc_prediction))\nprint('-------------Decision Tree--------------')\nDT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\nDT_prediction = DT.predict(X_test)\nprint(classification_report(y_test, DT_prediction))\nprint('-----------Random Forest----------------')\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\nRF_prediction = RF.predict(X_test)\nprint(classification_report(y_test, RF_prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that all three simple sklearn classifiers with default settings achieve higher accuracy than the default \"guess the most common one for everything\", with the SVM getting nearly to 60%.  This establishes a hard floor of performance, any complex model that fails to achieve at least 60% validation set accuracy is not worth using, as it can't even beat models that are essentially fact-free.","metadata":{}},{"cell_type":"markdown","source":"Alright, a bit more EDA before we get to the real models.  From the images seen above, we know that not all images are in the same orientation, even for the same patient.  Are there different orientations for different images within the same patient *and* same modality?  If so, we need to know this in order to properly do the alignment necessary for considering 3D structure.  Looking at the available information from the DICOM, the terms most likely to be relevant for determining positioning are:\n(0018, 1310) Acquisition Matrix \\\\\n(0018, 1314) Flip Angle  \\\\\n(0018, 5100) Patient Position \\\\\n(0020, 0037) Image Orientation (Patient) \\\\","metadata":{}},{"cell_type":"code","source":"#Access the JSON file with all of the sorted good images\nf = open('../input/brain-tumor-valid-images-in-order/all_valid_image_names.json')\ngood_image_names = json.load(f)\nprint('Number of Patients with good images: ', len(good_image_names))","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:54:15.054852Z","iopub.execute_input":"2021-09-16T19:54:15.055548Z","iopub.status.idle":"2021-09-16T19:54:15.152225Z","shell.execute_reply.started":"2021-09-16T19:54:15.05551Z","shell.execute_reply":"2021-09-16T19:54:15.15131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient = '00000'\ncodes = {'Acquisition_Matrix': ('0018', '1310'), 'Flip_Angle': ('0018', '1314'), 'Patient Position': ('0018', '5100'), \n         'Image Orientation (Patient)': ('0020', '0037')}\nmodes = ['FLAIR', 'T1wCE', 'T1w', 'T2w']\ndef count_orientations(patient, mode, good_images=good_image_names):\n    image_list = []\n    for im in good_images[patient][mode]:\n        image_list.append(glob.glob(f'{train_path}/train/{patient}/{mode}/{im}')[0])\n        \n    #print('Number of good images for patient %s in mode %s: %i' % (patient, mode, len(image_list)))\n    \n    code_lists = {k:[] for k in codes.keys()}\n    \n    for im in image_list:\n        dicom = pydicom.dcmread(im)\n        for c in codes.keys():\n            data = dicom[codes[c]].value\n            if type(data) == pydicom.multival.MultiValue or type(data) == list:\n                data = tuple(data)\n            code_lists[c].append(data)\n    \n    return code_lists\n\nfor m in modes:\n    print('-' * 25)\n    counts = count_orientations(patient, m)\n    for k,v in counts.items():\n        print('Number of distinct %s: %i' % (k, len(set(v))))\n        print('%s settings: ' % k)\n        print(set(v))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T20:24:19.230556Z","iopub.execute_input":"2021-09-16T20:24:19.230822Z","iopub.status.idle":"2021-09-16T20:24:20.636339Z","shell.execute_reply.started":"2021-09-16T20:24:19.230794Z","shell.execute_reply":"2021-09-16T20:24:20.635523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1):\n    _brats21id = train_df.iloc[i][\"BraTS21ID\"]\n    _mgmt_value = train_df.iloc[i][\"MGMT_value\"]\n    visualize_sample(brats21id=_brats21id, mgmt_value=_mgmt_value, slice_i=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:07:23.414616Z","iopub.execute_input":"2021-09-16T19:07:23.41488Z","iopub.status.idle":"2021-09-16T19:07:23.78306Z","shell.execute_reply.started":"2021-09-16T19:07:23.414852Z","shell.execute_reply":"2021-09-16T19:07:23.782373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient = '00000'\nmodes = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\nchosen_images = []\nfor m in modes:\n    image_names = good_image_names['00000'][m]\n    n_images = len(image_names)\n    floor = int(n_images * 0.25)\n    ceiling = int(n_images * 0.75)\n    i_slice = random.choice(range(floor, ceiling))\n\n    chosen_image = image_names[i_slice]\n    path = f'{train_path}/train/00000/{m}/{chosen_image}'\n    chosen_images.append(_dicom2array(path)) \n\nplt.figure(figsize=(16,5))\n\nfor i,m in enumerate(modes):\n    plt.subplot(1, 4, i+1)\n    plt.imshow(chosen_images[i], cmap=\"gray\")\n    plt.title(f\"{m}\", fontsize=16)\n    plt.axis(\"off\")\n\nplt.suptitle(f' Patient: 00000', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T20:13:12.001854Z","iopub.execute_input":"2021-09-16T20:13:12.002548Z","iopub.status.idle":"2021-09-16T20:13:12.437134Z","shell.execute_reply.started":"2021-09-16T20:13:12.002509Z","shell.execute_reply":"2021-09-16T20:13:12.436439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mismatch_counts = {m:0 for m in modes}\nfor p in good_image_names.keys():\n    print('Patient: ', p)\n    for m in good_image_names[p].keys():\n        counts = count_orientations(p, m)\n        for k,v in counts.items():\n            if len(set(v)) > 1:\n                print('Mismatch:  mode %s setting %s has %i distinct values' % (m, k, len(set(v))))\n                mismatch_counts[m] += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-16T20:25:57.630204Z","iopub.execute_input":"2021-09-16T20:25:57.630744Z","iopub.status.idle":"2021-09-16T20:36:05.415647Z","shell.execute_reply.started":"2021-09-16T20:25:57.630707Z","shell.execute_reply":"2021-09-16T20:36:05.413942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#These are the patients whose DICOM files, for at least one modality, are missing the \"slice location\" information\n#and therefore could not be sorted in the same way.\ncore_weirds = ['00109', '00157', '00170', '00186', '00353', '00367', '00414', '00561', '00563', '00564', '00565',\n         '00756', '00834', '00839']","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:27:49.997332Z","iopub.execute_input":"2021-09-15T21:27:49.997844Z","iopub.status.idle":"2021-09-15T21:27:50.003525Z","shell.execute_reply.started":"2021-09-15T21:27:49.997811Z","shell.execute_reply":"2021-09-15T21:27:50.002096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we have a fixed data set & each patient can be expensive to analyze in detail, we have produced a JSON file which has, as keys, the patient IDs, and values are dictionaries, where each dictionary has the modalities (FLAIR, etc) as keys and lists of image names as the values.  The image names within each list are in order according to the metadata in the corresponding DICOM files, where we have used 'slice location', accessed via this command: pydicom.dcmread(im)[('0020', '1041')].value.  However, there is a handful of patients with DICOM images that do not contain this information.  We will probably return to them later, but for the moment they have been classified as \"weird\" and are excluded from our initial training & analysis.","metadata":{}},{"cell_type":"markdown","source":"Several of these helper functions came from or were inspired by this notebook: https://www.kaggle.com/furcifer/no-baseline-pytorch-cnn-for-mri?scriptVersionId=68186710\n","metadata":{}},{"cell_type":"code","source":"#This is from the furcifer notebook\ndef _dicom2array(path, voi_lut=True, fix_monochrome=True, resize=False):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    #Normalize the data: subtract off the minimum, divide by the maximum, convert to 256 uint8\n    data = data - np.min(data)\n    data = data/np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    #Resize images to target value\n    if resize:\n        data = cv2.resize(data, (256, 256))\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:59:05.357856Z","iopub.execute_input":"2021-09-16T19:59:05.35856Z","iopub.status.idle":"2021-09-16T19:59:05.366939Z","shell.execute_reply.started":"2021-09-16T19:59:05.358523Z","shell.execute_reply":"2021-09-16T19:59:05.366313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To get an understanding of the what dcm files look like by manually changing the path\nimport matplotlib.pyplot as plt\nfrom pydicom import dcmread\nfrom pydicom.data import get_testdata_file\n\nds = dcmread('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-119.dcm')\n# plot the image using matplotlib\nplt.imshow(ds.pixel_array, cmap=plt.cm.gray)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If one looks at the raw images, one sees lots of blank space around the actually brain, which is not useful for classification.  The following function finds the edges of the brain and eliminates the surrounding blank space.  Note: this is circumscribing it, not eliminating *all* the blank space, there is still blank space in the corners because PyTorch needs a cuboid input (like putting something in a box, not shrinkwrapping it).","metadata":{}},{"cell_type":"code","source":"#Access the JSON file with all of the sorted good images\nf = open('../input/brain-tumor-valid-images-in-order/all_valid_image_names.json')\ngood_image_names = json.load(f)\nprint('Number of Patients with good images: ', len(good_image_names))","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:28:04.197601Z","iopub.execute_input":"2021-09-15T21:28:04.197909Z","iopub.status.idle":"2021-09-15T21:28:04.290444Z","shell.execute_reply.started":"2021-09-15T21:28:04.197879Z","shell.execute_reply":"2021-09-15T21:28:04.289468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(good_image_names['00000'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Some patients, for some modalities, only have a handful of images, which make them functionally unusable. \n#This cell loops through the good_image_names file, and returns a dictionary with the image modalities as \n#the keys and lists of \"underpopulated\" patients as the values\nthreshold = 20\nunderpopulated = {}\nmodes = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\nfor mode in modes:\n    sublist = [key for key, value in good_image_names.items() if len(value[mode]) < threshold]\n    underpopulated[mode] = sublist\n    \nprint('FLAIR underpopulated list: ', underpopulated['FLAIR'])\nprint('FLAIR underpopulated length: ', len(underpopulated['FLAIR']))\nprint('T1w underpopulated list: ', underpopulated['T1w'])\nprint('T1w underpopulated length: ', len(underpopulated['T1w']))\nprint('T1wCE underpopulated list: ', underpopulated['T1wCE'])\nprint('T1wCE underpopulated length: ', len(underpopulated['T1wCE']))\nprint('T2w underpopulated list: ', underpopulated['T2w'])\nprint('T2w underpopulated length: ', len(underpopulated['T2w']))\n\nfinal_exclusions = {'FLAIR': core_weirds + underpopulated['FLAIR'], 'T1w': core_weirds + underpopulated['T1w'], \n                   'T1wCE': core_weirds + underpopulated['T1wCE'], 'T2w': core_weirds + underpopulated['T2w']}\nprint('final exclusions: ', final_exclusions)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:28:11.4646Z","iopub.execute_input":"2021-09-15T21:28:11.464925Z","iopub.status.idle":"2021-09-15T21:28:11.485049Z","shell.execute_reply.started":"2021-09-15T21:28:11.464879Z","shell.execute_reply":"2021-09-15T21:28:11.482608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Eliminate as much blank space around a 2D slice as possible\ndef _circumscriber2D(img: np.array) -> np.array:\n    vmin = 0\n    hmin = 0\n    vlimit, hlimit = img.shape\n    \n    for i in range(vlimit):\n        if np.max(img[i, :]) == 0:\n            vmin += 1\n        else:\n            break\n    vmax = vmin + 1\n    for i in range(vmin+1, vlimit):\n        if np.max(img[i, :]) > 0:\n            vmax += 1\n        else:\n            break\n\n    for j in range(hlimit):\n        if np.max(img[:, j]) == 0:\n            hmin += 1\n        else:\n            break\n    hmax = hmin + 1\n    for j in range(hmin+1, hlimit):\n        if np.max(img[:, j]) > 0:\n            hmax += 1\n        else:\n            break\n    return img[vmin: vmax, hmin:hmax]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Eliminate as much blank space around a 3D cube as possible.  This one assumes the purely blank slices were already filtered out\n#Ensure each dimension has a minimum size...\ndef _circumscriber3D(img: np.array) -> np.array:\n    #First is vertical, second is horizontal, third is slices\n    vmin = 0\n    vlimit = img.shape[0]\n    hmin = 0\n    hlimit = img.shape[1]\n    \n    for i in range(vlimit):\n        if np.max(img[i, :, :]) == 0:\n            vmin += 1\n        else:\n            break\n    vmax = vmin + 1\n    for i in range(vmin+1, vlimit):\n        if np.max(img[i, :, :]) > 0:\n            vmax += 1\n        else:\n            break\n    \n    for j in range(hlimit):\n        if np.max(img[:, j, :]) == 0:\n            hmin += 1\n        else:\n            break\n    hmax = hmin + 1\n    for j in range(hmin+1, hlimit):\n        if np.max(img[:, j, :]) > 0:\n            hmax += 1\n        else:\n            break\n            \n    delta_v = max(vmax - vmin, 32)\n    delta_h = max(hmax - hmin, 32)\n    return img[vmin:vmin+delta_v, hmin:hmin+delta_h, :]","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:28:21.405957Z","iopub.execute_input":"2021-09-15T21:28:21.406263Z","iopub.status.idle":"2021-09-15T21:28:21.417562Z","shell.execute_reply.started":"2021-09-15T21:28:21.406233Z","shell.execute_reply":"2021-09-15T21:28:21.416418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the Cutout data augmenter: when active, it will randomly zero out cubes of the brain image.  My guess is that, with the proper block size ('side' parameter), this will be a more effective regularizer than SliceSkip, but I could be mistaken.\nBesides the size of the blocks, the number of dropped blocks will likely also matter a fair bit, but I'll add that functionality later.  I have changed the name from DropBlock to Cutout in keeping with the convention described in the DropBlock paper: https://arxiv.org/pdf/1810.12890.pdf\nThe full DropBlock regularizer applies this to all the feature maps produced by the convolutional layers, whereas what I'm doing here is making random changes to the input.","metadata":{}},{"cell_type":"code","source":"def _Cutout(img: np.array, side: int, hole_count: int) -> np.array:\n    ycap = img.shape[0] - side\n    xcap = img.shape[1] - side\n    \n    #Safety check\n    if ycap <= 5 or xcap <= 5:\n        return img\n    \n    nholes = random.randint(1, hole_count)\n    ycorners = random.sample(range(0, ycap), nholes)\n    xcorners = random.sample(range(0, xcap), nholes)\n    \n    if len(img.shape) == 3:\n        dcap = img.shape[2] - min(side, img.shape[2])\n        if dcap == 0:\n            dcorners = [0] * nholes\n        else:\n            dcorners = random.sample(range(0, dcap), nholes)\n    \n    for i in range(nholes):\n        if len(img.shape) == 3:\n            img[ycorners[i]:ycorners[i]+side, xcorners[i]:xcorners[i]+side, dcorners[i]:dcorners[i]+side] = 0\n        else:\n            img[ycorners[i]:ycorners[i]+side, xcorners[i]:xcorners[i]+side] = 0\n    \n    return img","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:28:30.425091Z","iopub.execute_input":"2021-09-15T21:28:30.425462Z","iopub.status.idle":"2021-09-15T21:28:30.440561Z","shell.execute_reply.started":"2021-09-15T21:28:30.425432Z","shell.execute_reply":"2021-09-15T21:28:30.439268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was copy-pasted from the furcifer notebook\ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(512,512)):\n    rows = len(imgs)//cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i in range(4):\n        img = imgs[:,:,i]\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's make use of the model developed here: https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/ \n#This should give us a sense of which slices in each brain contain the most relevant information, literally just measuring the \n#amount of abnormality in it.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HP_control = {'OPTIMIZER': 'Adam', 'IRL': 1e-3, 'SCHEDULER': 'StepLR', 'STEP_SIZE': 10, 'GAMMA': 0.1,\n             'HOLE_SIZE': 10, 'HOLE_LIMIT': 5, 'NUM_SLICES': 6, 'VAL_SPLIT': 0.25}","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:28:39.068588Z","iopub.execute_input":"2021-09-15T21:28:39.068885Z","iopub.status.idle":"2021-09-15T21:28:39.074874Z","shell.execute_reply.started":"2021-09-15T21:28:39.068854Z","shell.execute_reply":"2021-09-15T21:28:39.073722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Apply transformations for data augmentation\n#Not sure if RandomRotation & RandomHorizontalFlip are appropriate for 3D tensors.\n#The images are already normalized, don't really need that one either.\nchosen_transforms = {'train': transforms.Compose([\n    transforms.ToTensor(),\n    #transforms.RandomRotation(degrees=90),\n    #transforms.RandomHorizontalFlip(),\n    #transforms.Normalize(means, stds)\n]),\n   'val': transforms.Compose([\n       transforms.ToTensor(),\n       #transforms.Normalize(means, stds)\n   ])}","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:28:46.493761Z","iopub.execute_input":"2021-09-15T21:28:46.494117Z","iopub.status.idle":"2021-09-15T21:28:46.500143Z","shell.execute_reply.started":"2021-09-15T21:28:46.494086Z","shell.execute_reply":"2021-09-15T21:28:46.498814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was inspired by load_rand_dicom_images in the furcifer notebook\ndef load_FULL_brain(scan_id, split = 'train', modality='FLAIR', image_names=None, sliceSkip=0.0, cutOut=False):\n    \"\"\"\n    send all of the images in the chosen modality, in order, as a single 3D np array\n    \"\"\"\n    if split != \"train\" and split != \"test\":\n        print('Please request a valid split: train or test.  Defaulting to train.')\n        split = \"train\"\n        \n    if modality != 'FLAIR' and modality != 'T1w' and modality != 'T1wCE' and modality != 'T2w':\n        print('Please select an appropriate modality: FLAIR, T1w, T1wCE, or T2w')\n        print('Defaulting to FLAIR')\n        modality = 'FLAIR'\n        \n    if sliceSkip >= 1:\n        sliceSkip = 0\n        print('Please choose a valid sliceSkip number, from 0 to 1 inclusive/exclusive')\n        \n    if image_names:\n        image_block = []\n        #print('Patient ID: ', scan_id)\n        #print('Number of available images: ', len(image_names))\n        for im in image_names:\n            skipper = random.random()\n            if skipper < sliceSkip:\n                continue\n            image_block.append(glob.glob(f'{train_path}/{split}/{scan_id}/{modality}/{im}')[0])\n    \n        #print('Number of images in use: ', len(image_block))\n        real_images = [_dicom2array(f) for f in image_block]\n        good_images = np.array([im for im in real_images if np.max(im) > 0]).T #This filtering should be superfluous now\n        final_image = _circumscriber3D(good_images)\n        #Only use Cutout data augmentation if explicitly desired\n        if cutOut:\n            final_image = _Cutout(final_image, HP_control['HOLE_SIZE'], HP_control['HOLE_LIMIT'])\n            \n        return final_image\n        \n    print('Please input the valid image names in the proper order')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boxtest = load_FULL_brain('00000', split='train', modality='FLAIR', image_names=good_image_names['00000']['FLAIR'], \n                          sliceSkip=0.1, cutOut=True)\nplot_imgs(boxtest[:,:,100:104])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was inspired by the BrainTumor Dataset class in the furcifer notebook, though I have heavily modified it.\nclass BrainLoader(Dataset):\n    def __init__(self, label_file, path, split, modality, skip=0.0, cutout=False, good_images=None, exclude=None,\n                 val_split=0.25, transforms=None):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        splitdir = 'train'\n        self.split = split\n        if self.split == 'test':\n            splitdir = 'test'\n        self.modality = modality\n        \n        self.skip = skip\n        self.cutout = cutout\n        self.good_images = good_images\n        self.exclude = exclude\n        self.transform = transforms\n        if self.split != 'train':\n            self.skip = 0\n            self.drop = cutout\n        \n        self.ids = [a.split('/')[-1] for a in sorted(glob.glob(path + f'/{splitdir}/*'))]\n        self.ids = [a for a in self.ids if a not in self.exclude]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]    \n    \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        mode = self.modality\n        patient_images = self.good_images[p_id][mode]\n        \n        n_available = len(patient_images)\n        if n_available == 0:\n            print('There doesnt seem to be anything here! Lets go back and tell Master Luke...')\n            return None\n        \n        imgs = load_FULL_brain(p_id, split=self.split, modality=self.modality, image_names=patient_images, \n                               sliceSkip=self.skip, cutOut=self.cutout)\n        imgs = self.transform(imgs)\n        imgs = imgs.unsqueeze(0)\n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(imgs, dtype=torch.float32), label\n        return torch.tensor(imgs, dtype=torch.float32)\n\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_bs = 1\nval_bs = 1\nmode = 'T1w'\ntrain_dataset = BrainLoader('train_labels.csv', train_path, split='train', modality=mode, good_images=good_image_names, \n                            exclude=final_exclusions[mode], skip=0.0, cutout=True, transforms=chosen_transforms['train'])\nval_dataset = BrainLoader('train_labels.csv', train_path, split='val', modality=mode, good_images=good_image_names,\n                         exclude=final_exclusions[mode], skip=0.0, cutout=True, transforms=chosen_transforms['val'])\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=val_bs, shuffle=False)\n\ndataloaders = {'train': train_loader, 'val': val_loader}\nfor img, label in train_loader:\n    print('Iteration')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiSliceLoader(Dataset):\n    def __init__(self, label_file, path, modality, num_slices=1, split='train', good_images=None, exclude=None, \n                 val_split=0.25, transform=None, cutout=False):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        self.split = split\n        self.modality = modality\n        self.num_slices = num_slices\n        self.good_images = good_images\n        self.exclude = exclude\n        self.transform = transform\n        self.cutout = cutout\n        self.splitdir = 'train'\n        if self.split == 'test':\n            self.splitdir = 'test'\n        self.ids = [a.split('/')[-1] for a in sorted(glob.glob(path + f'/{self.splitdir}/*'))]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]\n            \n        self.ids = [a for a in self.ids if a not in self.exclude]\n    \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        mode = self.modality\n        patient_images = self.good_images[p_id][mode]\n        \n        n_available = len(patient_images)\n        if n_available == 0:\n            print('There doesnt seem to be anything here! Lets go back and tell Master Luke...')\n            return None\n        \n        if n_available < 3 * self.num_slices:\n            print('Not enough slices available for patient: ', p_id)\n            print('Number available: ', n_available)\n            print('Number requested: ', self.num_slices)\n            #image = load_FULL_brain(p_id, self.split, mode, self.cutout)\n            \n        floor = int(0.33 * n_available)\n        ceil = int(0.66 * n_available)\n        first = random.choice(range(floor, ceil))\n        #if n_available % 2 == 0:\n        #    first -= 1 \n        last = first + self.num_slices\n        \n        targets = patient_images[first:last]\n        target_paths = [f'{train_path}/{self.splitdir}/{p_id}/{mode}/{t}' for t in targets]\n        image = np.stack([_dicom2array(p) for p in target_paths]).T\n        \n        image = _circumscriber3D(image)\n        \n        if self.cutout:\n            image = _Cutout(image, HP_control['HOLE_SIZE'], HP_control['HOLE_LIMIT'])\n        \n        transform = self.transform\n        image = transform(image)\n        #print('image size: ', image.shape)\n        image = image.unsqueeze(0)\n        \n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(image, dtype=torch.float32), label\n        return torch.tensor(image, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:29:05.487191Z","iopub.execute_input":"2021-09-15T21:29:05.487522Z","iopub.status.idle":"2021-09-15T21:29:05.516812Z","shell.execute_reply.started":"2021-09-15T21:29:05.487466Z","shell.execute_reply":"2021-09-15T21:29:05.515748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a master dictionary to control all the hyperparameters for a particular model & experiment.  First we need to determine what all the possible hyperparameters actually *are*, we don't want to have to re-do this 100 times.","metadata":{}},{"cell_type":"code","source":"NUM_SLICES = HP_control['NUM_SLICES']","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:29:15.435998Z","iopub.execute_input":"2021-09-15T21:29:15.436389Z","iopub.status.idle":"2021-09-15T21:29:15.441063Z","shell.execute_reply.started":"2021-09-15T21:29:15.436343Z","shell.execute_reply":"2021-09-15T21:29:15.439965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's see if MultiSliceLoader works...\nmode = 'T1w'\ntrain_bs = 1\nval_bs = 1\ntrain_dataset = MultiSliceLoader('train_labels.csv', train_path, modality=mode, num_slices=NUM_SLICES, split='train',\n                                 good_images=good_image_names, exclude=final_exclusions[mode], val_split=HP_control['VAL_SPLIT'],\n                                 transform=chosen_transforms['train'])\nval_dataset = MultiSliceLoader('train_labels.csv', train_path, modality=mode, num_slices=NUM_SLICES, split='val',\n                              good_images=good_image_names, exclude=final_exclusions[mode],  val_split=HP_control['VAL_SPLIT'],\n                              transform=chosen_transforms['val'])\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=val_bs, shuffle=False)\n\ndataloaders = {'train': train_loader, 'val': val_loader}\n\nfor img, label in train_loader:\n    print('---------------Iteration---------------------')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break\n    \nfor img, label in val_loader:\n    print('---------------Iteration---------------------')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:29:24.554013Z","iopub.execute_input":"2021-09-15T21:29:24.554995Z","iopub.status.idle":"2021-09-15T21:29:24.93137Z","shell.execute_reply.started":"2021-09-15T21:29:24.554945Z","shell.execute_reply":"2021-09-15T21:29:24.930158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, now that we have the loader pretty well set up, we need an actual model or four to train.  This seems like a valuable source: https://pytorch.org/hub/ ","metadata":{}},{"cell_type":"markdown","source":"We want to modify the model to take an arbitrary number of slices as input channels. For a handful of adjacent slices, these are treated as different \"color channels\" of the same image, but for more than 3 or 4 we want to modify the actual structure of the model to use 3D convolutions instead.  Those models will be more complex but should be more effective because they'll take advantage of 3D structural information.  \nTo change the number of input channels, follow the instructions here: https://discuss.pytorch.org/t/how-to-modify-the-input-channels-of-a-resnet-model/2623/10","metadata":{}},{"cell_type":"markdown","source":"Alright, let's add a training loop, without a training loop we don't really have anything.","metadata":{}},{"cell_type":"code","source":"print('len(good_image_names): ', len(good_image_names))\nprint('len(underpopulated[mode]): ', len(underpopulated[mode]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's build a proper 3D CNN.  This is the encoder half of the U-Net used in a previous competition for identifying anomalous regions\n#in MRI brain scans, expanded into 3D.  References: https://arxiv.org/abs/1505.04597,\n#https://github.com/mateuszbuda/brain-segmentation-pytorch/blob/master/unet.py\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1, init_features=32):\n        super(Encoder, self).__init__()\n\n        features = init_features\n        self.encoder1 = Encoder._block(in_channels, features, name='enc1')\n        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=(1,2,2))\n        self.encoder2 = Encoder._block(features, features * 2, name='enc2')\n        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=(1,2,2))\n        self.encoder3 = Encoder._block(features * 2, features * 4, name='enc3')\n        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=(1,2,2))\n        self.encoder4 = Encoder._block(features * 4, features * 8, name='enc4') \n        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=(1,2,2))\n\n        #There may need to be something else between the final MaxPool layer and \n        #the global average pool layer...\n        self.avg = nn.AdaptiveAvgPool3d(1)\n        self.flat = nn.Flatten()\n        #fc1_in = features * 8 * NUM_SLICES \n        self.linear1 = nn.Linear(features * 8, 2)\n        #self.linear2 = nn.Linear(200, 2)\n\n    def forward(self, x):\n        #print('x size: ', x.size())\n        x = self.encoder1(x)\n        #print('enc1 size: ', x.size())\n        x = self.pool1(x)\n        #print('pool1 size: ', x.size())\n        x = self.encoder2(x)\n        #print('enc2 size: ', x.size())\n        x = self.pool2(x)\n        #print('pool3 size: ', x.size())\n        x = self.encoder3(x)\n        #print('enc3 size: ', x.size())\n        x = self.pool3(x)\n        #print('pool3 size: ', x.size())\n        x = self.encoder4(x)\n        #print('enc4 size: ', x.size())\n        x = self.pool4(x)\n        #print('pool4 size: ', x.size())\n        x = self.avg(x)\n        #x = F.adaptive_avg_pool3d(x, (1,1,1))\n        #print('Global pool size: ', x.size())\n        #x = x.view(x.shape[0],-1)\n        #x = self.flat(x)\n        #print('flat size: ', x.size())\n        x = x.squeeze()\n        #print('Squeezed size: ', x.size())\n        x = self.linear1(x)\n        #print('linear size: ', x.size())\n        x = x.unsqueeze(0)\n        #print('unsqueezed size: ', x.size())\n        #x = F.relu(self.linear1(x))\n        #x = self.linear2(x)\n        \n        return torch.sigmoid(x)\n\n    @staticmethod\n    def _block(in_channels, features, name):\n        return nn.Sequential(\n            OrderedDict([\n                         (\n                             name + 'conv1',\n                             nn.Conv3d(in_channels=in_channels,\n                                       out_channels=features,\n                                       kernel_size=3, \n                                       padding=1,\n                                       bias=False)\n                         ),\n                         (name + 'norm1', nn.BatchNorm3d(num_features=features)),\n                         (name + 'relu1', nn.ReLU(inplace=True)),\n                         (\n                             name + 'conv2',\n                             nn.Conv3d(in_channels=features,\n                                       out_channels=features,\n                                       kernel_size=3,\n                                       padding=1,\n                                       bias=False)\n                         ),\n                         (name + 'norm2', nn.BatchNorm3d(num_features=features)),\n                         (name + 'relu2', nn.ReLU(inplace=True))\n            ])\n        )\n","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:29:40.392079Z","iopub.execute_input":"2021-09-15T21:29:40.392379Z","iopub.status.idle":"2021-09-15T21:29:40.413811Z","shell.execute_reply.started":"2021-09-15T21:29:40.392349Z","shell.execute_reply":"2021-09-15T21:29:40.411257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc_model = Encoder()\n#test_input = torch.randn(1, 1, 100, 150, 200)\n#enc_model(test_input)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:29:51.051492Z","iopub.execute_input":"2021-09-15T21:29:51.051829Z","iopub.status.idle":"2021-09-15T21:29:51.102829Z","shell.execute_reply.started":"2021-09-15T21:29:51.051798Z","shell.execute_reply":"2021-09-15T21:29:51.101911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntrain_size = int((len(good_image_names) - len(underpopulated[mode])) * (1 - HP_control['VAL_SPLIT']))\nval_size = len(good_image_names) - len(underpopulated[mode]) - train_size\ndataset_sizes = {'train': train_size, 'val': val_size}\nprint('dataset_sizes: ', dataset_sizes)\ncriterion = nn.CrossEntropyLoss()\n#According to this page, the model should be sent to the device before setting the optimizer\n#https://pytorch.org/docs/stable/optim.html \n#model_resnet50 = model_resnet50.to(device)\n#Set a default optimizer\n#optimizer_ft = optim.Adam(model_resnet50.parameters(), lr=1e-4)\n\nenc_model = enc_model.to(device)\noptimizer_ft = optim.Adam(enc_model.parameters(), lr=1e-4)\n#If one is selected, overwrite it\nif HP_control['OPTIMIZER'] == 'Adam':\n    optimizer_ft = optim.Adam(enc_model.parameters(), lr=HP_control['IRL'])\n    #optimizer_ft = optim.Adam(model_resnet50.parameters(), lr=HP_control['IRL'])\n\nlr_scheduler = None\nif HP_control['SCHEDULER'] == 'StepLR':\n    # Decay LR by a factor of 0.1 every 5 epochs\n    lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=HP_control['STEP_SIZE'], gamma=HP_control['GAMMA'])\nelif HP_control['SCHEDULER'] == 'ExponentialLR':\n    # Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.\n    lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer_ft, gamma=HP_control['GAMMA'])","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:29:55.161599Z","iopub.execute_input":"2021-09-15T21:29:55.162524Z","iopub.status.idle":"2021-09-15T21:30:01.473432Z","shell.execute_reply.started":"2021-09-15T21:29:55.162459Z","shell.execute_reply":"2021-09-15T21:30:01.472337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs, device='cpu'):\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    print('Initiating training loop...')\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0\n            for img, label in dataloaders[phase]:\n                #Send everything to the relevant device\n                #print('Loading in new img and label')\n                img = img.to(device)\n                #label = label.float()\n                label = label.to(device)\n                \n                #Zero the parameter gradients\n                optimizer.zero_grad()\n                \n                #Forward pass, track history only in train mode\n                with torch.set_grad_enabled(phase == 'train'):\n                    output = model(img)\n                    #print('output: ', output)\n                    #print('max(output): ', max(output))\n                    #print('torch.argmax(output): ', torch.argmax(output))\n                    #_, pred = torch.max(output, dim=1)\n                    #_, pred = torch.max(output)\n                    pred = torch.argmax(output)\n                    loss = criterion(output, label)\n                 \n                \n                    #Backward pass, optimize only in training mode\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                # statistics\n                running_loss += loss.item() * img.size(0)\n                running_corrects += torch.sum(pred == label.data)\n            \n            if phase == 'train' and scheduler is not None:\n                scheduler.step()\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n            phase, epoch_loss, epoch_acc))\n            #print('{} Accuracy: {:.4f}'.format(phase, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n    \n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:30:11.77964Z","iopub.execute_input":"2021-09-15T21:30:11.779942Z","iopub.status.idle":"2021-09-15T21:30:11.793979Z","shell.execute_reply.started":"2021-09-15T21:30:11.779911Z","shell.execute_reply":"2021-09-15T21:30:11.79297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FT_model = train_model(model_resnet50, criterion, optimizer_ft, lr_scheduler, num_epochs=25, device=device)\nenc_model = train_model(enc_model, criterion, optimizer_ft, lr_scheduler, num_epochs=25, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-15T21:30:21.640542Z","iopub.execute_input":"2021-09-15T21:30:21.640854Z","iopub.status.idle":"2021-09-15T22:00:01.479039Z","shell.execute_reply.started":"2021-09-15T21:30:21.640825Z","shell.execute_reply":"2021-09-15T22:00:01.47756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's write our simplest cnn, then we can add variations or different models for improvement in results\n\nclass SimpleCNN(nn.Module): \n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=9) # 4 incoming channels\n        self.conv2 = nn.Conv2d(64, 32, kernel_size=7)\n        self.conv2_drop = nn.Dropout2d()\n        self.conv3 = nn.Conv2d(32, 16, kernel_size=5)\n        self.conv3_drop = nn.Dropout2d()\n        self.conv4 = nn.Conv2d(16, 8, kernel_size=3)\n        self.conv4_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(200, 256)\n        self.fc2 = nn.Linear(256, 2)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 4))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 4))\n        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n        x = x.view(x.shape[0],-1)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\nn_epochs = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)\nmodel(torch.randn(1, 4, 512, 512))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simpleCNN = train_model(model, criterion, optimizer, lr_scheduler, num_epochs=25, device=device)","metadata":{},"execution_count":null,"outputs":[]}]}