{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Participation in this competition helps contribute to a meaningful cause \nAs mentioned in the competition overview, \"If successful, you'll help brain cancer patients receive less invasive diagnoses and treatments. The introduction of new and customized treatment strategies before surgery has the potential to improve the management, survival, and prospects of patients with brain cancer.\"","metadata":{}},{"cell_type":"markdown","source":"**Dataset**\n\nThe dataset is divided as follows: Each independent case has a dedicated folder identified by a five-digit number. Within each of these “case” folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format. The exact mpMRI scans included are:\n\n* Fluid Attenuated Inversion Recovery (FLAIR)\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n\n**Files**\n\n* train/ - folder containing the training files, with each top-level folder representing a subject. \n* train_labels.csv - file containing the target MGMT_value for each subject in the training data (e.g. the presence of MGMT promoter methylation)\n* test/ - the test files, which use the same structure as train/; your task is to predict the MGMT_value for each subject in the test data. NOTE: the total size of the rerun test set (Public and Private) is ~5x the size of the Public test set\n* sample_submission.csv - a sample submission file in the correct format\n\nEach independent case is labeled with MGMT_value. 1 corresponds to the presence of MGMT (tumor) and 0 corresponds to absense.","metadata":{}},{"cell_type":"markdown","source":"## Understanding Dataset\n\nFor any Data Science project, it is crucial to understand the data, as best as possible, using visualization and Exploratory Data Analysis (EDA) techniques. \n\nLet's start with understanding the training labels file. But before that, let's make a cell to keep all the imported libraries in one place.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport copy\nimport cv2\nimport glob\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nfrom operator import itemgetter\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport random\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import models, transforms\nimport seaborn as sns\n\nrandom.seed(37)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-01T21:22:22.467762Z","iopub.execute_input":"2021-09-01T21:22:22.468147Z","iopub.status.idle":"2021-09-01T21:22:25.137453Z","shell.execute_reply.started":"2021-09-01T21:22:22.468071Z","shell.execute_reply":"2021-09-01T21:22:25.136603Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"EDA code taken from awesome work at https://www.kaggle.com/ihelon/brain-tumor-eda-with-animations-and-modeling","metadata":{}},{"cell_type":"code","source":"#exploring distribution of training lables file\ntrain_df = pd.read_csv(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv\")\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-08-30T22:44:37.488564Z","iopub.execute_input":"2021-08-30T22:44:37.488896Z","iopub.status.idle":"2021-08-30T22:44:37.517852Z","shell.execute_reply.started":"2021-08-30T22:44:37.488866Z","shell.execute_reply":"2021-08-30T22:44:37.516948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.countplot(data=train_df, x=\"MGMT_value\");","metadata":{"execution":{"iopub.status.busy":"2021-08-30T22:44:47.052525Z","iopub.execute_input":"2021-08-30T22:44:47.053239Z","iopub.status.idle":"2021-08-30T22:44:47.373396Z","shell.execute_reply.started":"2021-08-30T22:44:47.053195Z","shell.execute_reply":"2021-08-30T22:44:47.371591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\n\ndef visualize_sample(\n    brats21id, \n    slice_i,\n    mgmt_value,\n    types=(\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\")\n):\n    plt.figure(figsize=(16, 5))\n    patient_path = os.path.join(\n        \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\", \n        str(brats21id).zfill(5),\n    )\n    for i, t in enumerate(types, 1):\n        t_paths = sorted(\n            glob.glob(os.path.join(patient_path, t, \"*\")), \n            key=lambda x: int(x[:-4].split(\"-\")[-1]),\n        )\n        data = load_dicom(t_paths[int(len(t_paths) * slice_i)])\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{t}\", fontsize=16)\n        plt.axis(\"off\")\n\n    plt.suptitle(f\"MGMT_value: {mgmt_value}\", fontsize=16)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:40:29.05564Z","iopub.execute_input":"2021-08-30T19:40:29.056048Z","iopub.status.idle":"2021-08-30T19:40:29.069137Z","shell.execute_reply.started":"2021-08-30T19:40:29.056012Z","shell.execute_reply":"2021-08-30T19:40:29.067724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in random.sample(range(train_df.shape[0]), 10):\n    _brats21id = train_df.iloc[i][\"BraTS21ID\"]\n    _mgmt_value = train_df.iloc[i][\"MGMT_value\"]\n    visualize_sample(brats21id=_brats21id, mgmt_value=_mgmt_value, slice_i=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:40:39.064908Z","iopub.execute_input":"2021-08-30T19:40:39.065739Z","iopub.status.idle":"2021-08-30T19:40:43.720796Z","shell.execute_reply.started":"2021-08-30T19:40:39.065682Z","shell.execute_reply":"2021-08-30T19:40:43.719585Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification'","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:22:42.073118Z","iopub.execute_input":"2021-09-01T21:22:42.073446Z","iopub.status.idle":"2021-09-01T21:22:42.078000Z","shell.execute_reply.started":"2021-09-01T21:22:42.073418Z","shell.execute_reply":"2021-09-01T21:22:42.076703Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We have several ideas for how to proceed, both 2D and 3D versions.  In order to properly test the 2D ones, it would be useful to have a single reference of all the good (as in, not blank) images *in order*, so that random or midpoint selection can easily grab them without having to process everything.  Ideally we can save this in a JSON file, where the main keys are patient IDs and the interior keys are modalities, with the inner values being ordered lists.","metadata":{}},{"cell_type":"markdown","source":"Several of these helper functions came from or were inspired by this notebook: https://www.kaggle.com/furcifer/no-baseline-pytorch-cnn-for-mri?scriptVersionId=68186710\n","metadata":{}},{"cell_type":"code","source":"#This is from the furcifer notebook\ndef _dicom2array(path, voi_lut=True, fix_monochrome=True, resize=False):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    #Normalize the data: subtract off the minimum, divide by the maximum, convert to 256 uint8\n    data = data - np.min(data)\n    data = data/np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    #Resize images to target value\n    if resize:\n        data = cv2.resize(data, (256, 256))\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:22:49.400298Z","iopub.execute_input":"2021-09-01T21:22:49.400619Z","iopub.status.idle":"2021-09-01T21:22:49.408416Z","shell.execute_reply.started":"2021-09-01T21:22:49.400591Z","shell.execute_reply":"2021-09-01T21:22:49.407420Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#To get an understanding of the what dcm files look like by manually changing the path\nimport matplotlib.pyplot as plt\nfrom pydicom import dcmread\nfrom pydicom.data import get_testdata_file\n\nds = dcmread('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-119.dcm')\n# plot the image using matplotlib\nplt.imshow(ds.pixel_array, cmap=plt.cm.gray)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:23:09.927301Z","iopub.execute_input":"2021-09-01T21:23:09.927640Z","iopub.status.idle":"2021-09-01T21:23:10.105386Z","shell.execute_reply.started":"2021-09-01T21:23:09.927610Z","shell.execute_reply":"2021-09-01T21:23:10.104608Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"If one looks at the raw images, one sees lots of blank space around the actually brain, which is not useful for classification.  The following function finds the edges of the brain and eliminates the surrounding blank space.  Note: this is circumscribing it, not eliminating *all* the blank space, there is still blank space in the corners because PyTorch needs a cuboid input.","metadata":{}},{"cell_type":"code","source":"#This cell was used to create the JSON file containing all of the images per modality in order with the blanks stripped out\n#It also identified the \"weirds\", which are the patients where the DICOM images in at least one modality do not have the usual\n#complement of features.  I'm not sure what to do with them at this time, so for now they are being excluded.\n\nfull_data_dict = {}\nmodes = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\npatients = glob.glob(f'{train_path}/train/*')\npatient_IDs = sorted([p.split('/')[-1] for p in patients])\nweirds = ['00109', '00157', '00170', '00186', '00353', '00367', '00414', '00561', '00563', '00564', '00565',\n         '00756', '00834', '00839']\nprint('Number of Patients: ', len(patient_IDs))\nprint('First patient: ', patient_IDs[0])\nprint('Last patient: ', patient_IDs[-1])\nfor patient in patient_IDs:\n    if patient in weirds:\n        continue\n    print('Patient ID: ', patient)\n    subdict = {}\n    for mode in modes:\n        imgnames = glob.glob(f'{train_path}/train/{patient}/{mode}/*.dcm')\n        if len(imgnames) == 0:\n            subdict[mode] = []\n            continue\n        image_slice_locs = [pydicom.dcmread(im)[('0020', '1041')].value for im in imgnames]\n        imgpairs = zip(imgnames, image_slice_locs)\n        imgpairs = sorted(imgpairs, key=itemgetter(1))\n        \n        images = [f[0] for f in imgpairs]\n        #print('Number of initial %s images: %i' % (mode, len(images)))\n        real_images = [_dicom2array(f) for f in images]\n        good_indices = [idx for idx,image in enumerate(real_images) if np.max(image) > 0]\n        good_imnames = [im for idx,im in enumerate(images) if idx in good_indices]\n        if len(good_imnames) == 0:\n            subdict[mode] = []\n            continue\n        \n        final_imnames = [imname.split('/')[-1] for imname in good_imnames]\n    \n        #print('Number of final images: ', len(final_imnames))\n        subdict[mode] = final_imnames\n    full_data_dict[patient] = subdict","metadata":{"execution":{"iopub.status.busy":"2021-08-18T15:36:46.22051Z","iopub.execute_input":"2021-08-18T15:36:46.22113Z","iopub.status.idle":"2021-08-18T16:37:29.980109Z","shell.execute_reply.started":"2021-08-18T15:36:46.221085Z","shell.execute_reply":"2021-08-18T16:37:29.975826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is the cell that actually wrote the JSON file, using the dictionary from the above cell\n#with open('all_valid_image_names.json', 'w') as outfile:\n#    json.dump(full_data_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:30:23.068695Z","iopub.execute_input":"2021-08-18T20:30:23.069073Z","iopub.status.idle":"2021-08-18T20:30:23.09275Z","shell.execute_reply.started":"2021-08-18T20:30:23.069041Z","shell.execute_reply":"2021-08-18T20:30:23.091297Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Access the JSON file with all of the sorted good images\nf = open('../input/brain-tumor-valid-images-in-order/all_valid_image_names.json')\ngood_image_names = json.load(f)\nprint('Number of Patients with good images: ', len(good_image_names))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:23:19.946418Z","iopub.execute_input":"2021-09-01T21:23:19.946786Z","iopub.status.idle":"2021-09-01T21:23:20.063275Z","shell.execute_reply.started":"2021-09-01T21:23:19.946753Z","shell.execute_reply":"2021-09-01T21:23:20.062306Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Number of Patients with good images:  571\n","output_type":"stream"}]},{"cell_type":"code","source":"#Eliminate as much blank space around a 2D slice as possible\ndef _circumscriber2D(img: np.array) -> np.array:\n    vmin = 0\n    hmin = 0\n    vlimit, hlimit = img.shape\n    \n    for i in range(vlimit):\n        if np.max(img[i, :]) == 0:\n            vmin += 1\n        else:\n            break\n    vmax = vmin + 1\n    for i in range(vmin+1, vlimit):\n        if np.max(img[i, :]) > 0:\n            vmax += 1\n        else:\n            break\n\n    for j in range(hlimit):\n        if np.max(img[:, j]) == 0:\n            hmin += 1\n        else:\n            break\n    hmax = hmin + 1\n    for j in range(hmin+1, hlimit):\n        if np.max(img[:, j]) > 0:\n            hmax += 1\n        else:\n            break\n    return img[vmin: vmax, hmin:hmax]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:23:24.167580Z","iopub.execute_input":"2021-09-01T21:23:24.167918Z","iopub.status.idle":"2021-09-01T21:23:24.176380Z","shell.execute_reply.started":"2021-09-01T21:23:24.167871Z","shell.execute_reply":"2021-09-01T21:23:24.175115Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Eliminate as much blank space around a 3D cube as possible.  This one assumes the purely blank slices were already filtered out\ndef _circumscriber3D(img: np.array) -> np.array:\n    #First is vertical, second is horizontal, third is slices\n    vmin = 0\n    vlimit = img.shape[0]\n    hmin = 0\n    hlimit = img.shape[1]\n    \n    for i in range(vlimit):\n        if np.max(img[i, :, :]) == 0:\n            vmin += 1\n        else:\n            break\n    vmax = vmin + 1\n    for i in range(vmin+1, vlimit):\n        if np.max(img[i, :, :]) > 0:\n            vmax += 1\n        else:\n            break\n    \n    for j in range(hlimit):\n        if np.max(img[:, j, :]) == 0:\n            hmin += 1\n        else:\n            break\n    hmax = hmin + 1\n    for j in range(hmin+1, hlimit):\n        if np.max(img[:, j, :]) > 0:\n            hmax += 1\n        else:\n            break\n    return img[vmin:vmax, hmin:hmax, :]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:23:35.319195Z","iopub.execute_input":"2021-09-01T21:23:35.319525Z","iopub.status.idle":"2021-09-01T21:23:35.328654Z","shell.execute_reply.started":"2021-09-01T21:23:35.319495Z","shell.execute_reply":"2021-09-01T21:23:35.327761Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"This is the Cutout data augmenter: when active, it will randomly zero out cubes of the brain image.  My guess is that, with the proper block size ('side' parameter), this will be a more effective regularizer than SliceSkip, but I could be mistaken.\nBesides the size of the blocks, the number of dropped blocks will likely also matter a fair bit, but I'll add that functionality later.  I have changed the name from DropBlock to Cutout in keeping with the convention described in the DropBlock paper: https://arxiv.org/pdf/1810.12890.pdf\nThe full DropBlock regularizer applies this to all the feature maps produced by the convolutional layers, whereas what I'm doing here is making random changes to the input.","metadata":{}},{"cell_type":"code","source":"def _Cutout(img: np.array, side: int) -> np.array:\n    ycap = img.shape[0] - side\n    xcap = img.shape[1] - side\n    \n    nholes = random.randint(1, 5)\n    ycorners = random.sample(range(0, ycap), nholes)\n    xcorners = random.sample(range(0, xcap), nholes)\n    \n    if len(img.shape) == 3:\n        dcap = img.shape[2] - min(side, img.shape[2])\n        if dcap == 0:\n            dcorners = [0] * nholes\n        else:\n            dcorners = random.sample(range(0, dcap), nholes)\n    \n    for i in range(nholes):\n        if len(img.shape) == 3:\n            img[ycorners[i]:ycorners[i]+side, xcorners[i]:xcorners[i]+side, dcorners[i]:dcorners[i]+side] = 0\n        else:\n            img[ycorners[i]:ycorners[i]+side, xcorners[i]:xcorners[i]+side] = 0\n    \n    return img","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:31:05.736222Z","iopub.execute_input":"2021-09-01T22:31:05.736566Z","iopub.status.idle":"2021-09-01T22:31:05.744748Z","shell.execute_reply.started":"2021-09-01T22:31:05.736535Z","shell.execute_reply":"2021-09-01T22:31:05.743831Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def load_single_slice(patient_id, split='train', modality='FLAIR', position=0, cutout=False):\n    \"\"\"\n    Load in a single slice for a given patient, of a given modality and position within the line-up\n    \"\"\"\n    if split != 'train' and split != 'test':\n        print('Please request a valid split: train or test. Defaulting to train')\n        split='train'\n        \n    if modality not in ['FLAIR', 'T1w', 'T1wCE', 'T2w']:\n        print('Please choose a valid modality: FLAIR, T1w, T1wCE, or T2w. Defaulting to FLAIR')\n        modality = 'FLAIR'\n        \n    available_images = good_image_names[patient_id][modality]\n    if len(available_images) == 0:\n        print('There doesnt seem to be anything here! Lets go back and tell Master Luke...')\n        print('Patient ID: ', patient_id)\n        print('Modality: ', modality)\n        return None\n    \n    if position == 'middle':\n        position = int(len(available_images) * 0.5)\n    \n    #print('Number of Available Images: ', len(available_images))\n    if position >= len(available_images):\n        print('Please choose an acceptable position, must be no more than: ', len(available_images) - 1)\n        print('Defaulting to middle position')\n        position = int(len(available_images) * 0.5)\n\n    imname = available_images[position]\n    #print('imname: ', imname)\n    image_path = f'{train_path}/{split}/{patient_id}/{modality}/{imname}'\n    image = glob.glob(image_path)[0]\n    real_image = _dicom2array(image)\n    #print('real_image shape: ', real_image.shape)\n    final_image = _circumscriber2D(real_image).T\n    #print('final_image shape: ', final_image.shape)\n    if cutout:\n        final_image = _Cutout(final_image, 5)\n    return final_image\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:24:10.181419Z","iopub.execute_input":"2021-09-01T21:24:10.181736Z","iopub.status.idle":"2021-09-01T21:24:10.190678Z","shell.execute_reply.started":"2021-09-01T21:24:10.181706Z","shell.execute_reply":"2021-09-01T21:24:10.189761Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#This was inspired by load_rand_dicom_images in the furcifer notebook\ndef load_FULL_brain(scan_id, split = 'train', modality='FLAIR', sliceSkip=0.0, cutOut=False):\n    \"\"\"\n    send all of the images in the chosen modality, in order, as a single 3D np array\n    \"\"\"\n    if split != \"train\" and split != \"test\":\n        print('Please request a valid split: train or test.  Defaulting to train.')\n        split = \"train\"\n        \n    if modality != 'FLAIR' and modality != 'T1w' and modality != 'T1wCE' and modality != 'T2w':\n        print('Please select an appropriate modality: FLAIR, T1w, T1wCE, or T2w')\n        print('Defaulting to FLAIR')\n        modality = 'FLAIR'\n        \n    if sliceSkip >= 1:\n        sliceSkip = 0\n        print('Please choose a valid sliceSkip number, from 0 to 1 inclusive/exclusive')\n        \n    image = sorted(glob.glob(f'{train_path}/{split}/{scan_id}/{modality}/*.dcm'))\n    nKeep = int(len(image) * (1 - sliceSkip))\n    image = random.sample(image, nKeep)\n    \n    image_slice_locs = [pydicom.dcmread(im)[('0020', '1041')].value for im in image]\n    image_pairs = list(zip(image, image_slice_locs))\n    ordered_IP = sorted(image_pairs, key=itemgetter(1))\n    images = [f[0] for f in ordered_IP]\n    real_images = [_dicom2array(f) for f in images]\n    good_images = np.array([im for im in real_images if np.max(im) > 0]).T\n    final_image = _circumscriber3D(good_images)\n    \n    #Only use Cutout data augmentation if explicitly desired\n    if cutOut:\n        final_image = _Cutout(final_image, 10)\n    \n    return final_image","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:24:28.472412Z","iopub.execute_input":"2021-09-01T21:24:28.472732Z","iopub.status.idle":"2021-09-01T21:24:28.484497Z","shell.execute_reply.started":"2021-09-01T21:24:28.472702Z","shell.execute_reply":"2021-09-01T21:24:28.483617Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#This was copy-pasted from the furcifer notebook\ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(512,512)):\n    rows = len(imgs)//cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i in range(4):\n        img = imgs[:,:,i]\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:43:58.511528Z","iopub.execute_input":"2021-08-30T19:43:58.51218Z","iopub.status.idle":"2021-08-30T19:43:58.518878Z","shell.execute_reply.started":"2021-08-30T19:43:58.512138Z","shell.execute_reply":"2021-08-30T19:43:58.517963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boxtest = load_FULL_brain('00000', split='train', modality='FLAIR', sliceSkip=0.0, cutOut=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:44:12.463415Z","iopub.execute_input":"2021-08-30T19:44:12.464068Z","iopub.status.idle":"2021-08-30T19:44:18.612967Z","shell.execute_reply.started":"2021-08-30T19:44:12.464027Z","shell.execute_reply":"2021-08-30T19:44:18.611875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at four slices from the middle of patient 00000's FLAIR brain image.","metadata":{}},{"cell_type":"code","source":"plot_imgs(boxtest[:,:,100:104])","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:44:28.420376Z","iopub.execute_input":"2021-08-30T19:44:28.42079Z","iopub.status.idle":"2021-08-30T19:44:29.550367Z","shell.execute_reply.started":"2021-08-30T19:44:28.420752Z","shell.execute_reply":"2021-08-30T19:44:29.549308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SliceLoader(Dataset):\n    def __init__(self, label_file, path, split, modality, position=None, exclude=None, val_split=0.25,\n                transform = None, cutout=False):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        self.split = split\n        self.modality = modality\n        self.position = position\n        self.exclude = exclude\n        self.transform = transform\n        self.cutout = cutout\n        splitdir = 'train'\n        if self.split == 'test':\n            splitdir = 'test'\n        self.ids = [a.split('/')[-1] for a in sorted(glob.glob(path + f'/{splitdir}/*'))]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]\n            \n        self.ids = [a for a in self.ids if a not in self.exclude]\n            \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        if not self.position:\n            available_images = good_image_names[p_id][self.modality]\n            position = int(len(available_images) * 0.5) #Grab the middle slice if one not selected\n        else:\n            position = self.position\n            \n        if self.split == 'val':\n            self.split = 'train'\n        image = load_single_slice(p_id, self.split, self.modality, position, self.cutout)\n        image_triple = np.stack([image, image, image]).T\n        transform = self.transform\n        image = transform(image_triple)\n        \n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(image, dtype=torch.float32), label\n        return torch.tensor(image, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:24:57.567265Z","iopub.execute_input":"2021-09-01T21:24:57.567595Z","iopub.status.idle":"2021-09-01T21:24:57.583534Z","shell.execute_reply.started":"2021-09-01T21:24:57.567567Z","shell.execute_reply":"2021-09-01T21:24:57.582652Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class MultiSliceLoader(Dataset):\n    def __init__(self, label_file, path, modality, num_slices=1, split='train', good_images=None, exclude=None, \n                 val_split=0.25, transform=None, cutout=False):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        self.split = split\n        self.modality = modality\n        self.num_slices = num_slices\n        self.good_images = good_images\n        self.exclude = exclude\n        self.transform = transform\n        self.cutout = cutout\n        self.splitdir = 'train'\n        if self.split == 'test':\n            self.splitdir = 'test'\n        self.ids = [a.split('/')[-1] for a in sorted(glob.glob(path + f'/{self.splitdir}/*'))]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]\n            \n        self.ids = [a for a in self.ids if a not in self.exclude]\n    \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        mode = self.modality\n        patient_images = self.good_images[p_id][mode]\n        \n        n_available = len(patient_images)\n        if n_available == 0:\n            print('There doesnt seem to be anything here! Lets go back and tell Master Luke...')\n            return None\n        \n        if n_available < self.num_slices:\n            image = load_FULL_brain()\n            \n        first = int(n_available * 0.5) - int(self.num_slices * 0.5)\n        last = int(n_available * 0.5) + int(self.num_slices * 0.5)\n        if n_available % 2 == 0:\n            first -= 1 \n        \n        targets = patient_images[first:last]\n        target_paths = [f'{train_path}/{self.splitdir}/{p_id}/{mode}/{t}' for t in targets]\n        image = np.stack([_dicom2array(p) for p in target_paths]).T\n        if self.cutout:\n            image = _Cutout(image, 5)\n        \n        transform = self.transform\n        image = transform(image)\n        \n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(image, dtype=torch.float32), label\n        return torch.tensor(image, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:36:30.654686Z","iopub.execute_input":"2021-09-01T22:36:30.655036Z","iopub.status.idle":"2021-09-01T22:36:30.672310Z","shell.execute_reply.started":"2021-09-01T22:36:30.655004Z","shell.execute_reply":"2021-09-01T22:36:30.671476Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"weirds = ['00109', '00157', '00170', '00186', '00353', '00367', '00414', '00561', '00563', '00564', '00565', '00709',\n         '00756', '00834', '00839']","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:31:50.195595Z","iopub.execute_input":"2021-09-01T22:31:50.195934Z","iopub.status.idle":"2021-09-01T22:31:50.200361Z","shell.execute_reply.started":"2021-09-01T22:31:50.195898Z","shell.execute_reply":"2021-09-01T22:31:50.199195Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Let's see if MultiSliceLoader works...\nNUM_SLICES = 5\ntrain_bs = 1\ntrain_dataset = MultiSliceLoader('train_labels.csv', train_path, modality='FLAIR', num_slices=NUM_SLICES, split='train',\n                                 good_images=good_image_names, exclude=weirds, transform=chosen_transforms['train'])\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n\nfor img, label in train_loader:\n    print('---------------Iteration---------------------')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:43:04.475800Z","iopub.execute_input":"2021-09-01T22:43:04.476160Z","iopub.status.idle":"2021-09-01T22:43:04.648445Z","shell.execute_reply.started":"2021-09-01T22:43:04.476129Z","shell.execute_reply":"2021-09-01T22:43:04.647393Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"---------------Iteration---------------------\ntorch.Size([1, 5, 480, 640])\ntensor(-1.9867)\ntensor(-1.4871)\ntensor(2.4381)\ntorch.Size([1])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","output_type":"stream"}]},{"cell_type":"code","source":"#Normalization terms, because we are using a model pretrained on ImageNet\n#mean_nums = [0.485, 0.456, 0.406]\n#std_nums = [0.229, 0.224, 0.225]\npure_mean = 0.449\npure_std = 0.226\nmeans = [pure_mean] * NUM_SLICES\nstds = [pure_std] * NUM_SLICES\n\n#Apply transformations for data augmentation\nchosen_transforms = {'train': transforms.Compose([\n    transforms.ToTensor(),\n    transforms.RandomRotation(degrees=90),\n    transforms.RandomHorizontalFlip(),\n    transforms.Normalize(means, stds)\n]),\n   'val': transforms.Compose([\n       transforms.ToTensor(),\n       transforms.Normalize(means, stds)\n   ])}","metadata":{"execution":{"iopub.status.busy":"2021-09-01T22:42:53.966847Z","iopub.execute_input":"2021-09-01T22:42:53.967198Z","iopub.status.idle":"2021-09-01T22:42:53.974509Z","shell.execute_reply.started":"2021-09-01T22:42:53.967168Z","shell.execute_reply":"2021-09-01T22:42:53.973324Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Let's see if SliceLoader can actually load slices\ntrain_bs = 1\ntrain_dataset = SliceLoader('train_labels.csv', train_path, split='train', modality='FLAIR', position=17, exclude=weirds, \n                           transform=chosen_transforms['train'])\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n\nfor img, label in train_loader:\n    print('---------------Iteration---------------------')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-08-30T20:08:28.411469Z","iopub.execute_input":"2021-08-30T20:08:28.412019Z","iopub.status.idle":"2021-08-30T20:08:28.481458Z","shell.execute_reply.started":"2021-08-30T20:08:28.411982Z","shell.execute_reply":"2021-08-30T20:08:28.48068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = pd.read_csv(os.path.join(train_path, 'train_labels.csv'))\nlabels.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T22:49:04.542877Z","iopub.execute_input":"2021-08-30T22:49:04.543219Z","iopub.status.idle":"2021-08-30T22:49:04.558343Z","shell.execute_reply.started":"2021-08-30T22:49:04.543185Z","shell.execute_reply":"2021-08-30T22:49:04.557626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was inspired by the BrainTumor Dataset class in the furcifer notebook, though I have heavily modified it.\nclass BrainLoader(Dataset):\n    def __init__(self, label_file, path, split, modality, skip=0.0, cutout=False, val_split=0.25):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        splitdir = 'train'\n        self.split = split\n        if self.split == 'test':\n            splitdir = 'test'\n        self.modality = modality\n        \n        self.skip = skip\n        self.cutout = cutout\n        if self.split != 'train':\n            self.skip = 0\n            self.drop = cutout\n        \n        self.ids = [a.split('/')[-1] for a in sorted(glob.glob(path + f'/{splitdir}/*'))]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]\n            \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        imgs = load_FULL_brain(p_id, split=self.split, modality=self.modality, sliceSkip=self.skip, cutOut=self.cutout)\n        transform = transforms.Compose([transforms.ToTensor()])\n        imgs = transform(imgs)\n        \n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(imgs, dtype=torch.float32), label\n        return torch.tensor(imgs, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:45:38.943023Z","iopub.execute_input":"2021-08-30T19:45:38.943436Z","iopub.status.idle":"2021-08-30T19:45:38.958257Z","shell.execute_reply.started":"2021-08-30T19:45:38.943405Z","shell.execute_reply":"2021-08-30T19:45:38.957056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's test out BrainLoader.","metadata":{}},{"cell_type":"code","source":"train_bs = 1\ntrain_dataset = BrainLoader('train_labels.csv', train_path, split='train', modality='T1w', skip=0.0, cutout=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n\nfor img, label in train_loader:\n    print('Iteration')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-08-26T19:34:39.121821Z","iopub.execute_input":"2021-08-26T19:34:39.122129Z","iopub.status.idle":"2021-08-26T19:34:40.825669Z","shell.execute_reply.started":"2021-08-26T19:34:39.122098Z","shell.execute_reply":"2021-08-26T19:34:40.824792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, now that we have the loader pretty well set up, we need an actual model or four to train.  This seems like a valuable source: https://pytorch.org/hub/ ","metadata":{}},{"cell_type":"code","source":"#Simple Resnet-18 model, pretrained on ImageNet.  Let's see what it can do...\nmodel_dummy = models.resnet18(pretrained=True)\nnum_ftrs = model_dummy.fc.in_features\n# Here the size of each output sample is set to 2.\nmodel_dummy.fc = nn.Linear(num_ftrs, 2)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T19:34:43.317983Z","iopub.execute_input":"2021-08-26T19:34:43.318432Z","iopub.status.idle":"2021-08-26T19:34:46.159276Z","shell.execute_reply.started":"2021-08-26T19:34:43.318396Z","shell.execute_reply":"2021-08-26T19:34:46.158382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet50 = models.resnet50(pretrained=True)\nnum_ftrs = model_resnet50.fc.in_features\nmodel_resnet50.fc = nn.Linear(num_ftrs, 2)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:26:16.024764Z","iopub.execute_input":"2021-09-01T21:26:16.025100Z","iopub.status.idle":"2021-09-01T21:26:19.247155Z","shell.execute_reply.started":"2021-09-01T21:26:16.025070Z","shell.execute_reply":"2021-09-01T21:26:19.246337Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9beafcabd9754959b07044a21e851591"}},"metadata":{}}]},{"cell_type":"markdown","source":"We want to modify the model to take an arbitrary number of slices as input channels. For a handful of adjacent slices, these are treated as different \"color channels\" of the same image, but for more than 3 or 4 we want to modify the actual structure of the model to use 3D convolutions instead.  Those models will be more complex but should be more effective because they'll take advantage of 3D structural information.  \nTo change the number of input channels, follow the instructions here: https://discuss.pytorch.org/t/how-to-modify-the-input-channels-of-a-resnet-model/2623/10","metadata":{}},{"cell_type":"code","source":"print(model_resnet50)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:31:06.447132Z","iopub.execute_input":"2021-09-01T21:31:06.447526Z","iopub.status.idle":"2021-09-01T21:31:06.453968Z","shell.execute_reply.started":"2021-09-01T21:31:06.447495Z","shell.execute_reply":"2021-09-01T21:31:06.453020Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=2, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"layer = model_resnet50.conv1\n\n# Creating new Conv2d layer\nnew_layer = nn.Conv2d(in_channels=NUM_SLICES, \n                  out_channels=layer.out_channels, \n                  kernel_size=layer.kernel_size, \n                  stride=layer.stride, \n                  padding=layer.padding,\n                  bias=layer.bias)\n\ncopy_weights = 0 # Here will initialize the weights from new channel with the red channel weights\n\n# Copying the weights from the old to the new layer\nnew_layer.weight[:, :layer.in_channels, :, :] = layer.weight.clone()\n\n#Copying the weights of the `copy_weights` channel of the old layer to the extra channels of the new layer\nfor i in range(NUM_SLICES - layer.in_channels):\n    channel = layer.in_channels + i\n    new_layer.weight[:, channel:channel+1, :, :] = layer.weight[:, copy_weights:copy_weights+1, : :].clone()\nnew_layer.weight = nn.Parameter(new_layer.weight)\n\nmodel_resnet50.conv1 = new_layer","metadata":{"execution":{"iopub.status.busy":"2021-09-01T21:36:59.178075Z","iopub.execute_input":"2021-09-01T21:36:59.178501Z","iopub.status.idle":"2021-09-01T21:36:59.188740Z","shell.execute_reply.started":"2021-09-01T21:36:59.178467Z","shell.execute_reply":"2021-09-01T21:36:59.187918Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_bs = 1\nval_bs = 1\ntrain_dummy = SliceLoader('train_labels.csv', train_path, split='train', modality='FLAIR', position='middle', exclude=weirds,\n                         transform = chosen_transforms['train'], cutout=False)\nval_dummy = SliceLoader('train_labels.csv', train_path, split='val', modality='FLAIR', position='middle', exclude=weirds,\n                       transform = chosen_transforms['val'])\ntrain_loader = DataLoader(train_dummy, batch_size=train_bs, shuffle=True)\nval_loader = DataLoader(val_dummy, batch_size=val_bs, shuffle=False)\n\ndataloaders = {'train': train_loader, 'val': val_loader}\n\nn_images = 0\nn_rand = 0\nn_correct = 0\n\nfor img, label in val_loader:\n    n_images += 1\n    if n_images > 5:\n        break\n    #print('img shape: ', img.shape)\n\n    out = model_resnet50(img)\n    print('out: ', out)\n    _,pred = torch.max(out, dim=1)\n    randpred = random.choice([0,1])\n    print('pred: ', pred)\n    print('out[0][pred]: ', out[0][pred])\n    if pred == label:\n        n_correct += 1\n    if randpred == label:\n        n_rand += 1\n        \nprint('Correct: ', n_correct)\nprint('Random Correct: ', n_rand)\nprint('Total: ', n_images)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T23:08:55.382381Z","iopub.execute_input":"2021-08-30T23:08:55.382735Z","iopub.status.idle":"2021-08-30T23:08:57.184671Z","shell.execute_reply.started":"2021-08-30T23:08:55.382694Z","shell.execute_reply":"2021-08-30T23:08:57.183829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, let's add a training loop, without a training loop we don't really have anything.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndataset_sizes = {'train': 427, 'val': 143}\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.Adam(model_resnet50.parameters(), lr=1e-4)\n# Decay LR by a factor of 0.1 every 5 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)\nmodel_resnet50 = model_resnet50.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T23:09:09.089382Z","iopub.execute_input":"2021-08-30T23:09:09.089712Z","iopub.status.idle":"2021-08-30T23:09:09.139469Z","shell.execute_reply.started":"2021-08-30T23:09:09.089679Z","shell.execute_reply":"2021-08-30T23:09:09.138642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs, device='cpu'):\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0\n            for img, label in dataloaders[phase]:\n                #Send everything to the relevant device\n                img = img.to(device)\n                #label = label.float()\n                label = label.to(device)\n                \n                #Zero the parameter gradients\n                optimizer.zero_grad()\n                \n                #Forward pass, track history only in train mode\n                with torch.set_grad_enabled(phase == 'train'):\n                    output = model(img)\n                    _, pred = torch.max(output, dim=1)\n                    loss = criterion(output, label)\n                 \n                \n                    #Backward pass, optimize only in training mode\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                # statistics\n                running_loss += loss.item() * img.size(0)\n                running_corrects += torch.sum(pred == label.data)\n            \n            if phase == 'train':\n                scheduler.step()\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n            phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n    \n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-30T23:09:14.277974Z","iopub.execute_input":"2021-08-30T23:09:14.278323Z","iopub.status.idle":"2021-08-30T23:09:14.289656Z","shell.execute_reply.started":"2021-08-30T23:09:14.278286Z","shell.execute_reply":"2021-08-30T23:09:14.28868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FT_model = train_model(model_resnet50, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T23:09:22.144536Z","iopub.execute_input":"2021-08-30T23:09:22.144866Z","iopub.status.idle":"2021-08-30T23:26:59.238695Z","shell.execute_reply.started":"2021-08-30T23:09:22.144837Z","shell.execute_reply":"2021-08-30T23:26:59.237216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}